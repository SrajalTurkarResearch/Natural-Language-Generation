{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Style Text Generator with Emotion Sliders\n",
        "\n",
        "This notebook implements a multi-style text generator with emotion sliders using a pre-trained transformer (GPT-2) augmented with LoRA for style conditioning and soft prompts for emotion control. The system allows users to generate text (e.g., reviews, responses) with adjustable styles (formal, casual) and emotions (joy, sadness).\n",
        "\n",
        "## Setup\n",
        "- **Model**: GPT-2 Small (117M parameters) for efficiency.\n",
        "- **Libraries**: Hugging transformers, peft (for LoRA), torch.\n",
        "- **Functionality**: Users select a style via LoRA adapters and adjust emotion sliders to modulate output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch transformers peft",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import numpy as np\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Define LoRA for style conditioning (formal, casual)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of LoRA matrices\n",
        "    lora_alpha=32,\n",
        "    target_modules=['c_attn', 'c_proj'],\n",
        "    lora_dropout=0.1,\n",
        "    bias='none'\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Simulated LoRA weights (in practice, train these)\n",
        "style_weights = {\n",
        "    'formal': torch.randn(model.config.n_embd, 8),  # Dummy weights\n",
        "    'casual': torch.randn(model.config.n_embd, 8)\n",
        "}\n",
        "\n",
        "# Soft prompt for emotion control (joy, sadness)\n",
        "class SoftPrompt(torch.nn.Module):\n",
        "    def __init__(self, embed_size, prompt_length=10):\n",
        "        super().__init__()\n",
        "        self.prompt = torch.nn.Parameter(torch.randn(prompt_length, embed_size))\n",
        "    def forward(self):\n",
        "        return self.prompt\n",
        "\n",
        "# Initialize soft prompts for emotions\n",
        "embed_size = model.config.n_embd\n",
        "soft_prompt_joy = SoftPrompt(embed_size)\n",
        "soft_prompt_sadness = SoftPrompt(embed_size)\n",
        "\n",
        "# Function to combine emotions based on slider values\n",
        "def get_emotion_prompt(joy_val, sadness_val):\n",
        "    # Normalize slider values (-1 to 1)\n",
        "    joy_val = max(min(joy_val, 1), -1)\n",
        "    sadness_val = max(min(sadness_val, 1), -1)\n",
        "    # Weighted combination\n",
        "    weight_joy = (joy_val + 1) / 2  # Map to [0,1]\n",
        "    weight_sadness = (sadness_val + 1) / 2\n",
        "    total = weight_joy + weight_sadness\n",
        "    if total == 0:\n",
        "        total = 1  # Avoid division by zero\n",
        "    combined_prompt = (weight_joy * soft_prompt_joy() + weight_sadness * soft_prompt_sadness()) / total\n",
        "    return combined_prompt\n",
        "\n",
        "# Generation function\n",
        "def generate_text(prompt, style='formal', joy_val=0.0, sadness_val=0.0, max_length=50):\n",
        "    # Set LoRA weights for style (simulated)\n",
        "    # In practice, load trained LoRA adapters\n",
        "    # Apply style-specific LoRA (placeholder)\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    input_ids = inputs['input_ids']\n",
        "    \n",
        "    # Get emotion soft prompt\n",
        "    emotion_prompt = get_emotion_prompt(joy_val, sadness_val)\n",
        "    \n",
        "    # Concatenate soft prompt embeddings with input\n",
        "    embeddings = model.get_input_embeddings()(input_ids)\n",
        "    prompt_embeds = torch.cat([emotion_prompt.unsqueeze(0), embeddings], dim=1)\n",
        "    \n",
        "    # Generate\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs_embeds=prompt_embeds,\n",
        "            max_length=max_length,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "prompt = 'Write a review for a smartphone.'\n",
        "print('Formal, Joyful Review:')\n",
        "print(generate_text(prompt, style='formal', joy_val=0.8, sadness_val=-0.2))\n",
        "print('\\nCasual, Sad Review:')\n",
        "print(generate_text(prompt, style='casual', joy_val=-0.5, sadness_val=0.7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- **Training**: In a real implementation, train LoRA adapters on style-specific datasets (e.g., formal academic texts, casual social media posts) and soft prompts on emotion-labeled data (e.g., GoEmotions).\n",
        "- **UI Integration**: For sliders, deploy via a web app (e.g., Flask + HTML sliders) calling `generate_text`.\n",
        "- **Limitations**: Current code uses dummy LoRA weights; actual training required for production."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}