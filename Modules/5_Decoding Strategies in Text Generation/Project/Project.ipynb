{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Simulator for Language Models\n",
    "\n",
    "This notebook implements a decoding simulator to compare **greedy search**, **beam search**, **top-k sampling**, and **top-p sampling** with **temperature scaling**. It visualizes token paths, computes **entropy** and **diversity** metrics, and analyzes the **diversity vs. coherence** tradeoff, inspired by OpenAI's work (Holtzman et al., 2019).\n",
    "\n",
    "## Setup\n",
    "Install dependencies and load the GPT-2 model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch numpy networkx matplotlib ipywidgets\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Define functions for probability, entropy, diversity, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log probability\n",
    "def get_log_prob(logits, token_id):\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    log_probabilities = torch.log(probabilities)\n",
    "    return log_probabilities[token_id].item()\n",
    "\n",
    "# Compute entropy\n",
    "def compute_entropy(logits):\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return -torch.sum(probabilities * torch.log(probabilities + 1e-10)).item()\n",
    "\n",
    "# Compute diversity (unique bigrams)\n",
    "def compute_diversity(texts):\n",
    "    bigrams = set()\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigrams.add((tokens[i], tokens[i + 1]))\n",
    "    return len(bigrams)\n",
    "\n",
    "# Visualize token paths\n",
    "def plot_graph(graph, length, title):\n",
    "    plt.figure(figsize=(8, 6), dpi=300, facecolor='white')\n",
    "    pos = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n",
    "    scores = [data['tokenscore'] for _, data in graph.nodes(data=True) if data['token'] is not None]\n",
    "    vmin, vmax = min(scores), max(scores)\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = LinearSegmentedColormap.from_list('rg', [\"r\", \"y\", \"g\"], N=256)\n",
    "    \n",
    "    nx.draw_networkx_nodes(graph, pos, node_size=2000, node_shape='o', alpha=1, \n",
    "                          linewidths=4, node_color=scores, cmap=cmap)\n",
    "    nx.draw_networkx_edges(graph, pos)\n",
    "    labels = {node: f\"{data['token'].split('_')[0]}\\n{data['tokenscore']:.2f}%\\nH={data['entropy']:.2f}\" \n",
    "              for node, data in graph.nodes(data=True) if data['token'] is not None}\n",
    "    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=8)\n",
    "    \n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, orientation='vertical', label='Token probability (%)')\n",
    "    plt.title(title)\n",
    "    plt.box(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategies\n",
    "Implement greedy search, beam search, top-k sampling, and top-p sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Search\n",
    "def greedy_search(input_ids, node, graph, length=5):\n",
    "    if length == 0:\n",
    "        return input_ids\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits[0, -1, :]\n",
    "    token_id = torch.argmax(logits).unsqueeze(0)\n",
    "    token_score = get_log_prob(logits, token_id)\n",
    "    new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n",
    "    next_token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
    "    current_node = list(graph.successors(node))[0]\n",
    "    graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n",
    "    graph.nodes[current_node]['token'] = next_token + f\"_{length}\"\n",
    "    graph.nodes[current_node]['entropy'] = compute_entropy(logits)\n",
    "    return greedy_search(new_input_ids, current_node, graph, length - 1)\n",
    "\n",
    "# Beam Search\n",
    "def beam_search(input_ids, length=5, num_beams=3):\n",
    "    beams = [(input_ids, 0.0, [0])]\n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_node(0, token='Start', tokenscore=100, entropy=0.0)\n",
    "    \n",
    "    for step in range(length):\n",
    "        new_beams = []\n",
    "        for beam_ids, beam_score, node_path in beams:\n",
    "            outputs = model(beam_ids)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "            top_k = torch.topk(logits, num_beams, dim=-1)\n",
    "            for i, token_id in enumerate(top_k.indices):\n",
    "                new_score = beam_score + get_log_prob(logits, token_id)\n",
    "                new_ids = torch.cat([beam_ids, token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "                new_node = len(graph.nodes)\n",
    "                next_token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
    "                graph.add_node(new_node, token=next_token + f\"_{length-step}\", \n",
    "                              tokenscore=np.exp(get_log_prob(logits, token_id)) * 100,\n",
    "                              entropy=compute_entropy(logits))\n",
    "                graph.add_edge(node_path[-1], new_node)\n",
    "                new_beams.append((new_ids, new_score, node_path + [new_node]))\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:num_beams]\n",
    "    \n",
    "    return beams[0][0], graph\n",
    "\n",
    "# Top-k Sampling\n",
    "def top_k_sampling(input_ids, node, graph, length=5, k=5, temperature=1.0):\n",
    "    if length == 0:\n",
    "        return input_ids\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits[0, -1, :] / temperature\n",
    "    top_k = torch.topk(logits, k, dim=-1)\n",
    "    probs = torch.nn.functional.softmax(top_k.values, dim=-1)\n",
    "    token_idx = torch.multinomial(probs, 1)\n",
    "    token_id = top_k.indices[token_idx].unsqueeze(0)\n",
    "    token_score = get_log_prob(logits, token_id)\n",
    "    new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n",
    "    next_token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
    "    current_node = list(graph.successors(node))[0]\n",
    "    graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n",
    "    graph.nodes[current_node]['token'] = next_token + f\"_{length}\"\n",
    "    graph.nodes[current_node]['entropy'] = compute_entropy(logits)\n",
    "    return top_k_sampling(new_input_ids, current_node, graph, length - 1, k, temperature)\n",
    "\n",
    "# Top-p Sampling\n",
    "def top_p_sampling(input_ids, node, graph, length=5, p=0.9, temperature=1.0):\n",
    "    if length == 0:\n",
    "        return input_ids\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits[0, -1, :] / temperature\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    nucleus = cumulative_probs <= p\n",
    "    nucleus_indices = sorted_indices[nucleus]\n",
    "    nucleus_probs = torch.nn.functional.softmax(sorted_logits[nucleus], dim=-1)\n",
    "    token_idx = torch.multinomial(nucleus_probs, 1)\n",
    "    token_id = nucleus_indices[token_idx].unsqueeze(0)\n",
    "    token_score = get_log_prob(logits, token_id)\n",
    "    new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n",
    "    next_token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
    "    current_node = list(graph.successors(node))[0]\n",
    "    graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n",
    "    graph.nodes[current_node]['token'] = next_token + f\"_{length}\"\n",
    "    graph.nodes[current_node]['entropy'] = compute_entropy(logits)\n",
    "    return top_p_sampling(new_input_ids, current_node, graph, length - 1, p, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Simulator\n",
    "Use widgets to adjust parameters and run the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive widgets\n",
    "prompt_widget = widgets.Text(value='The cat is', description='Prompt:')\n",
    "length_widget = widgets.IntSlider(value=5, min=3, max=10, description='Length:')\n",
    "k_widget = widgets.IntSlider(value=5, min=1, max=20, description='Top-k:')\n",
    "p_widget = widgets.FloatSlider(value=0.9, min=0.1, max=1.0, step=0.1, description='Top-p:')\n",
    "temp_widget = widgets.FloatSlider(value=1.0, min=0.1, max=2.0, step=0.1, description='Temperature:')\n",
    "beams_widget = widgets.IntSlider(value=3, min=1, max=5, description='Beams:')\n",
    "run_button = widgets.Button(description='Run Simulator')\n",
    "\n",
    "def run_simulator(b):\n",
    "    prompt = prompt_widget.value\n",
    "    length = length_widget.value\n",
    "    k = k_widget.value\n",
    "    p = p_widget.value\n",
    "    temperature = temp_widget.value\n",
    "    num_beams = beams_widget.value\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    outputs = []\n",
    "    \n",
    "    # Greedy Search\n",
    "    graph = nx.balanced_tree(1, length, create_using=nx.DiGraph())\n",
    "    for node in graph.nodes:\n",
    "        graph.nodes[node]['tokenscore'] = 100\n",
    "        graph.nodes[node]['token'] = prompt\n",
    "        graph.nodes[node]['entropy'] = 0.0\n",
    "    output_ids = greedy_search(input_ids, 0, graph, length)\n",
    "    greedy_text = tokenizer.decode(output_ids.squeeze(), skip_special_tokens=True)\n",
    "    outputs.append(('Greedy', greedy_text, graph))\n",
    "    \n",
    "    # Beam Search\n",
    "    output_ids, graph = beam_search(input_ids, length, num_beams)\n",
    "    beam_text = tokenizer.decode(output_ids.squeeze(), skip_special_tokens=True)\n",
    "    outputs.append(('Beam', beam_text, graph))\n",
    "    \n",
    "    # Top-k Sampling\n",
    "    graph = nx.balanced_tree(1, length, create_using=nx.DiGraph())\n",
    "    for node in graph.nodes:\n",
    "        graph.nodes[node]['tokenscore'] = 100\n",
    "        graph.nodes[node]['token'] = prompt\n",
    "        graph.nodes[node]['entropy'] = 0.0\n",
    "    output_ids = top_k_sampling(input_ids, 0, graph, length, k, temperature)\n",
    "    topk_text = tokenizer.decode(output_ids.squeeze(), skip_special_tokens=True)\n",
    "    outputs.append((f'Top-k (k={k}, T={temperature})', topk_text, graph))\n",
    "    \n",
    "    # Top-p Sampling\n",
    "    graph = nx.balanced_tree(1, length, create_using=nx.DiGraph())\n",
    "    for node in graph.nodes:\n",
    "        graph.nodes[node]['tokenscore'] = 100\n",
    "        graph.nodes[node]['token'] = prompt\n",
    "        graph.nodes[node]['entropy'] = 0.0\n",
    "    output_ids = top_p_sampling(input_ids, 0, graph, length, p, temperature)\n",
    "    topp_text = tokenizer.decode(output_ids.squeeze(), skip_special_tokens=True)\n",
    "    outputs.append((f'Top-p (p={p}, T={temperature})', topp_text, graph))\n",
    "    \n",
    "    # Compute diversity\n",
    "    texts = [text for _, text, _ in outputs]\n",
    "    diversity = compute_diversity(texts)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    for name, text, graph in outputs:\n",
    "        avg_entropy = np.mean([data['entropy'] for _, data in graph.nodes(data=True) if data['entropy'] > 0])\n",
    "        print(f\"\\n{name} Output: {text}\")\n",
    "        print(f\"Average Entropy: {avg_entropy:.2f}\")\n",
    "        plot_graph(graph, length, name)\n",
    "    print(f\"\\nDiversity (unique bigrams): {diversity}\")\n",
    "\n",
    "run_button.on_click(run_simulator)\n",
    "display(prompt_widget, length_widget, k_widget, p_widget, temp_widget, beams_widget, run_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "- **Greedy Search**: High coherence, low diversity, low entropy.\n",
    "- **Beam Search**: Improved coherence, moderate diversity.\n",
    "- **Top-k/Top-p Sampling**: High diversity, potential incoherence with high temperature.\n",
    "- **Tradeoffs**: Sampling methods excel in creative tasks; search methods suit factual tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}