{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comprehensive Tutorial on Evaluation Metrics in Natural Language Generation (NLG)\n",
    "\n",
    "Welcome to this interactive Jupyter Notebook tutorial on **Evaluation Metrics in Natural Language Generation (NLG)**! As an aspiring scientist, you're taking a critical step toward mastering NLG, a key area in natural language processing (NLP). This tutorial is designed for beginners, assuming no prior knowledge, and covers everything from theory to practical code, visualizations, real-world applications, research directions, and project ideas. We'll use Python libraries like `nltk`, `rouge_score`, and `bert_score` to compute metrics and `matplotlib`/`seaborn` for visualizations.\n",
    "\n",
    "## Objectives\n",
    "- Understand NLG and why evaluation metrics matter.\n",
    "- Learn human and automatic evaluation metrics (BLEU, ROUGE, METEOR, BERTScore, etc.).\n",
    "- Implement metrics with Python code.\n",
    "- Visualize results to compare models.\n",
    "- Explore real-world applications, case studies, and project ideas.\n",
    "- Discover research directions, rare insights, and tips for scientists.\n",
    "- Identify gaps in the tutorial and next steps for your career.\n",
    "\n",
    "## Prerequisites\n",
    "- Install Python and Jupyter Notebook.\n",
    "- Install libraries: `pip install nltk rouge-score bert-score matplotlib seaborn transformers torch datasets`.\n",
    "- Basic Python knowledge (we'll guide you through the rest!).\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction to NLG and Evaluation Metrics\n",
    "2. Human Evaluation\n",
    "3. Automatic Evaluation Metrics\n",
    "4. Practical Code Guides\n",
    "5. Visualizations\n",
    "6. Real-World Applications\n",
    "7. Mini and Major Project Ideas\n",
    "8. Research Directions and Rare Insights\n",
    "9. Future Directions and Next Steps\n",
    "10. Tips for Aspiring Scientists\n",
    "11. What We Didn’t Cover (Essential for Scientists)\n",
    "12. Conclusion\n",
    "\n",
    "**Note**: Run each code cell by clicking the \"Run\" button or pressing `Shift+Enter`. Take notes as you go, and feel free to experiment with the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to NLG and Evaluation Metrics\n",
    "\n",
    "### What is NLG?\n",
    "NLG is the process of generating human-like text using computational models. Examples include chatbots, automated report generators, and creative writing tools. Think of NLG as a robot author crafting a story or answering questions.\n",
    "\n",
    "**Analogy**: NLG is like a chef cooking a dish (the text). The ingredients are words, and the recipe is the model’s logic. We need to check if the dish tastes good (fluent), looks appealing (coherent), and meets the diner’s needs (relevant).\n",
    "\n",
    "### Why Evaluate NLG?\n",
    "Evaluation metrics help us measure:\n",
    "- **Accuracy**: Is the information correct?\n",
    "- **Fluency**: Does it read naturally?\n",
    "- **Relevance**: Does it address the task?\n",
    "- **Diversity**: Is it creative and varied?\n",
    "\n",
    "Metrics are like a scorecard for the chef’s dish, guiding improvements and comparisons.\n",
    "\n",
    "### Types of Metrics\n",
    "1. **Human Evaluation**: Humans score text based on criteria like fluency.\n",
    "2. **Automatic Metrics**: Algorithms compute scores (e.g., BLEU, ROUGE).\n",
    "\n",
    "**Analogy**: Human evaluation is a food critic tasting the dish. Automatic metrics are a machine analyzing ingredients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Human Evaluation\n",
    "\n",
    "### What is Human Evaluation?\n",
    "Humans read and score generated text based on criteria like fluency, coherence, relevance, informativeness, and creativity. It’s the gold standard because humans understand context and nuance.\n",
    "\n",
    "### Criteria\n",
    "- **Fluency**: Grammatical correctness and naturalness.\n",
    "- **Coherence**: Logical flow.\n",
    "- **Relevance**: Matches the task.\n",
    "- **Informativeness**: Provides useful information.\n",
    "- **Creativity**: Originality and engagement.\n",
    "\n",
    "**Example**:\n",
    "- *Generated Text*: “This phone is awesome with a great camera and super fast.”\n",
    "- *Scores*:\n",
    "  - Fluency: 4/5 (informal but natural).\n",
    "  - Coherence: 5/5 (logical).\n",
    "  - Informativeness: 3/5 (lacks details).\n",
    "  - Relevance: 5/5 (matches task).\n",
    "\n",
    "### Pros and Cons\n",
    "**Pros**:\n",
    "- Captures nuances like tone or humor.\n",
    "- Reflects user experience.\n",
    "**Cons**:\n",
    "- Subjective and costly.\n",
    "- Hard to scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic Evaluation Metrics\n",
    "\n",
    "Automatic metrics are fast, scalable algorithms. They include:\n",
    "\n",
    "### Word-Based Metrics\n",
    "#### BLEU\n",
    "- **Measures**: N-gram overlap with reference text.\n",
    "- **Range**: 0–1 (higher = better).\n",
    "- **Analogy**: Checks if your dish uses the same ingredients as a reference recipe.\n",
    "\n",
    "#### ROUGE\n",
    "- **Measures**: Recall of n-grams or longest common subsequences.\n",
    "- **Variants**: ROUGE-N, ROUGE-L.\n",
    "- **Analogy**: Checks how many reference ingredients you included.\n",
    "\n",
    "#### METEOR\n",
    "- **Measures**: Matches exact words, synonyms, and stems, plus word order.\n",
    "- **Analogy**: Allows substitute ingredients (e.g., basil for parsley) and checks arrangement.\n",
    "\n",
    "### Embedding-Based Metrics\n",
    "#### BERTScore\n",
    "- **Measures**: Semantic similarity using BERT embeddings.\n",
    "- **Range**: 0 to 1 (higher = better).\n",
    "- **Analogy**: Compares the flavor profile of dishes.\n",
    "\n",
    "#### MoverScore\n",
    "- **Measures**: Semantic distance using Word Mover’s Distance.\n",
    "- **Analogy**: Measures effort to transform one dish’s flavors into another’s.\n",
    "\n",
    "### Other Metrics\n",
    "#### Perplexity\n",
    "- **Measures**: Fluency (lower = more predictable).\n",
    "- **Analogy**: Grades how naturally you speak.\n",
    "\n",
    "#### Distinct-n\n",
    "- **Measures**: Diversity (unique n-grams).\n",
    "- **Analogy**: Counts unique spices in a dish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Practical Code Guides\n",
    "# Let's compute BLEU, ROUGE, and BERTScore for sample texts.\n",
    "\n",
    "# Import libraries\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Sample texts\n",
    "reference = \"The cat is on the mat\"\n",
    "generated = \"The cat sits on the mat\"\n",
    "\n",
    "# BLEU Score\n",
    "ref_tokens = [reference.split()]\n",
    "gen_tokens = generated.split()\n",
    "bleu = sentence_bleu(ref_tokens, gen_tokens, weights=(0.5, 0.5))\n",
    "print(f\"BLEU Score: {bleu:.3f}\")\n",
    "\n",
    "# ROUGE Score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = scorer.score(reference, generated)\n",
    "print(f\"ROUGE-1: {rouge_scores['rouge1'].recall:.3f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores['rougeL'].recall:.3f}\")\n",
    "\n",
    "# BERTScore\n",
    "refs = [reference]\n",
    "cands = [generated]\n",
    "P, R, F1 = bert_score(cands, refs, lang=\"en\", verbose=True)\n",
    "print(f\"BERTScore F1: {F1.mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations\n",
    "\n",
    "Let's compare metrics for two models across multiple texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for two models\n",
    "data = {\n",
    "    'Model': ['Model A', 'Model A', 'Model A', 'Model B', 'Model B', 'Model B'],\n",
    "    'Metric': ['BLEU', 'ROUGE-1', 'BERTScore', 'BLEU', 'ROUGE-1', 'BERTScore'],\n",
    "    'Score': [0.75, 0.80, 0.90, 0.65, 0.85, 0.88]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Model', y='Score', hue='Metric', data=df)\n",
    "plt.title('Model Comparison: Evaluation Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Applications\n",
    "\n",
    "- **Chatbots**: Evaluate responses for customer support (e.g., relevance, fluency).\n",
    "- **Summarization**: Assess news or document summaries (e.g., ROUGE for content overlap).\n",
    "- **Machine Translation**: Compare translated text to reference translations (e.g., BLEU).\n",
    "- **Creative Writing**: Measure diversity and creativity in story generation (e.g., Distinct-n).\n",
    "\n",
    "**Example**: A news agency uses ROUGE to evaluate an NLG system that summarizes articles, ensuring key information is retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Mini and Major Project Ideas\n",
    "\n",
    "### Mini Project: Chatbot Response Evaluator\n",
    "- **Task**: Build a Python script to evaluate chatbot responses using BLEU and BERTScore.\n",
    "- **Steps**:\n",
    "  1. Collect 10 chatbot responses and human-written references.\n",
    "  2. Compute BLEU and BERTScore.\n",
    "  3. Visualize scores in a bar plot.\n",
    "- **Code Snippet**:\n",
    "```python\n",
    "refs = [\"How can I help you today?\"]\n",
    "cands = [\"What can I do for you?\"]\n",
    "from bert_score import score as bert_score\n",
    "P, R, F1 = bert_score(cands, refs, lang=\"en\")\n",
    "print(f\"BERTScore F1: {F1.mean().item():.3f}\")\n",
    "```\n",
    "\n",
    "### Major Project: NLG Model Benchmarking\n",
    "- **Task**: Compare multiple NLG models (e.g., GPT-2, T5) on a summarization task.\n",
    "- **Steps**:\n",
    "  1. Use a dataset (e.g., CNN/DailyMail from `datasets` library).\n",
    "  2. Generate summaries with each model.\n",
    "  3. Compute BLEU, ROUGE, BERTScore, and human evaluations.\n",
    "  4. Analyze which model performs best and why.\n",
    "- **Code Snippet**:\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0', split='test[:10]')\n",
    "summarizer = pipeline('summarization', model='t5-small')\n",
    "summaries = [summarizer(article['article'], max_length=50)[0]['summary_text'] for article in dataset]\n",
    "refs = [article['highlights'] for article in dataset]\n",
    "```\n",
    "\n",
    "## 8. Research Directions and Rare Insights\n",
    "\n",
    "### Research Directions\n",
    "- **Context-Aware Metrics**: Develop metrics that consider user intent or dialogue context.\n",
    "- **Human-Metric Alignment**: Improve automatic metrics to better match human judgments.\n",
    "- **Multimodal NLG**: Evaluate text combined with images or audio.\n",
    "- **Fairness and Bias**: Create metrics to detect bias in generated text.\n",
    "\n",
    "### Rare Insights\n",
    "- **BLEU’s Limitations**: BLEU penalizes valid paraphrases (e.g., “big” vs. “large”), making it less suitable for creative tasks. Recent studies suggest combining BLEU with BERTScore for better coverage.\n",
    "- **Human Bias**: Human evaluators can be inconsistent due to personal biases (e.g., preferring formal language). Standardizing criteria is crucial.\n",
    "- **Emerging Metrics**: Metrics like BLEURT (Google, 2020) use fine-tuned language models for better semantic understanding but are less accessible due to complexity.\n",
    "\n",
    "## 9. Future Directions and Next Steps\n",
    "\n",
    "### Future Directions\n",
    "- **Unified Metrics**: Develop a single metric combining fluency, coherence, and relevance.\n",
    "- **Real-Time Evaluation**: Create metrics for live NLG systems (e.g., chatbots).\n",
    "- **Domain-Specific Metrics**: Tailor metrics for medical, legal, or creative NLG.\n",
    "\n",
    "### Next Steps\n",
    "- **Practice**: Run the code above and experiment with different texts.\n",
    "- **Read**: Papers like “BLEU” (Papineni et al., 2002) or “BERTScore” (Zhang et al., 2020).\n",
    "- **Experiment**: Fine-tune a model like T5 and evaluate its outputs.\n",
    "- **Join Communities**: Engage with NLP communities on platforms like GitHub or Hugging Face.\n",
    "\n",
    "## 10. Tips for Aspiring Scientists\n",
    "- **Combine Metrics**: Use multiple metrics for a holistic view.\n",
    "- **Validate with Humans**: Always include human evaluation for critical tasks.\n",
    "- **Stay Updated**: Follow conferences like ACL or EMNLP for new metrics.\n",
    "- **Document Code**: Keep clear notebooks for reproducible research.\n",
    "- **Ask Why**: Understand why a metric gives a certain score to improve your model.\n",
    "\n",
    "## 11. What We Didn’t Cover (Essential for Scientists)\n",
    "The previous tutorial covered core metrics but missed:\n",
    "- **Advanced Metrics**: BLEURT, PARENT (for abstractive summarization).\n",
    "- **Statistical Significance**: Use t-tests to compare model performance.\n",
    "- **Error Analysis**: Manually inspect low-scoring texts to identify model weaknesses.\n",
    "- **Domain Adaptation**: Tailoring metrics for specific fields (e.g., medical NLG).\n",
    "- **Ethical Considerations**: Evaluating fairness and bias in text.\n",
    "\n",
    "**Why It Matters**: As a scientist, you’ll need to analyze statistical significance, adapt metrics to domains, and ensure ethical outputs. These topics require deeper study through research papers and datasets.\n",
    "\n",
    "## 12. Conclusion\n",
    "You’ve learned NLG evaluation metrics through theory, code, and projects! Practice with the code, explore datasets, and read research papers to deepen your understanding. As a scientist, your ability to evaluate and improve NLG systems will set you apart."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}