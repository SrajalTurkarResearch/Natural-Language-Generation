{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-Memory Transformers in Natural Language Generation (NLG) - A Comprehensive Tutorial\n",
    "\n",
    "Welcome, aspiring scientist! This Jupyter Notebook is your complete guide to understanding **Long-Memory Transformers** in **Natural Language Generation (NLG)**. Designed for beginners yet rigorous for researchers, it covers everything from fundamentals to advanced concepts, with practical code, visualizations, applications, and research insights. Think of this as your scientific blueprint—like Turing’s code-breaking, Einstein’s relativity, or Tesla’s inventions—structured for note-taking and sparking your career.\n",
    "\n",
    "## Why This Tutorial?\n",
    "- **Your Goal**: To become a scientist and researcher.\n",
    "- **Approach**: Starts from scratch, uses simple analogies (e.g., memory as a library), includes math with full calculations, and provides code, visualizations, and projects.\n",
    "- **Structure**: Clear sections for theory, code, exercises, and research directions, so you can write notes and understand the logic.\n",
    "- **Extras**: Addresses gaps in standard tutorials (e.g., why long-memory matters for science) and includes case studies (in a separate .md file).\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python (lists, loops).\n",
    "- No prior NLP knowledge needed—we’ll build from the ground up.\n",
    "- Install libraries: `pip install torch transformers numpy matplotlib`.\n",
    "\n",
    "Let’s embark on this journey to master long-memory transformers and advance your scientific career!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Fundamentals of Transformers and NLG\n",
    "\n",
    "### 1.1 What is Natural Language Generation (NLG)?\n",
    "- **Definition**: NLG is AI generating human-like text from data or prompts. Think of it as a robot storyteller turning raw info into sentences.\n",
    "- **Logic**: Computers predict words based on patterns in data, aiming for coherence (makes sense) and relevance (fits context).\n",
    "- **Real-World Examples**:\n",
    "  - Chatbots (e.g., Siri) answering questions.\n",
    "  - Email auto-complete suggesting replies.\n",
    "  - AI summarizing news from datasets.\n",
    "- **Why for Scientists?**: NLG automates report writing, hypothesis generation, or dialogue simulation for experiments.\n",
    "\n",
    "### 1.2 What are Transformers?\n",
    "- **Analogy**: Transformers are like detectives solving a mystery (text processing) by looking at all clues (words) at once, unlike older models (RNNs) that read sequentially.\n",
    "- **Theory**: Introduced in 2017 (\"Attention Is All You Need\"), transformers use **attention** to weigh important words, processing text in parallel for speed.\n",
    "- **Components**:\n",
    "  - **Encoder**: Reads input, creates rich representations (like summarizing a book).\n",
    "  - **Decoder**: Generates output word by word, using encoder’s notes.\n",
    "  - **Attention**: Calculates relevance between words.\n",
    "- **Math of Self-Attention**:\n",
    "  - Input: Words as vectors, X = [x₁, x₂, ..., xₙ].\n",
    "  - Compute: Queries (Q = X * Wq), Keys (K = X * Wk), Values (V = X * Wv), where W are learnable matrices.\n",
    "  - Attention: `softmax(Q * Kᵀ / √dₖ) * V`, where dₖ is key dimension.\n",
    "  - Logic: Scores how much each word ‘attends’ to others, like voting on importance.\n",
    "\n",
    "#### Example Calculation\n",
    "For words \"cat\" ([1,0]) and \"sat\" ([0,1]), with Wq=Wk=Wv=identity, dₖ=2:\n",
    "- Q = K = V = [[1,0], [0,1]].\n",
    "- Q * Kᵀ = [[1,0], [0,1]].\n",
    "- Divide by √2 ≈ 1.41: [[0.71,0], [0,0.71]].\n",
    "- Softmax: ≈[[0.67,0.33], [0.33,0.67]].\n",
    "- Attention = Softmax * V = weighted vectors.\n",
    "- **Logic**: \"Cat\" attends 67% to itself, 33% to \"sat.\"\n",
    "\n",
    "#### Visualization\n",
    "- **Sketch**: Draw a box for encoder (stacked layers), decoder (similar), connected by arrows. Attention as lines between words, thicker for higher scores.\n",
    "- **Code**: Below, we’ll plot an attention heatmap."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple attention visualization\n",
    "words = ['cat', 'sat']\n",
    "attention_scores = np.array([[0.67, 0.33], [0.33, 0.67]])\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(attention_scores, cmap='hot', interpolation='nearest')\n",
    "plt.xticks(np.arange(len(words)), words)\n",
    "plt.yticks(np.arange(len(words)), words)\n",
    "plt.colorbar(label='Attention Score')\n",
    "plt.title('Self-Attention Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Explanation: Red=strong attention, blue=weak. Diagonal shows self-attention."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Limitations of Standard Transformers\n",
    "- **Problem**: Quadratic complexity (O(n²) time/space for n words) limits handling long texts (e.g., books).\n",
    "- **Analogy**: Like a notepad with limited pages—you can’t store a novel’s plot.\n",
    "- **Logic**: Gradients weaken over distance (vanishing gradients), causing **recency bias** (focus on recent words).\n",
    "- **Math**: Information dependence drops as 1/d (d=distance), so long-range gradients ≈0.\n",
    "- **NLG Issue**: Generating long stories loses coherence if early context is forgotten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Long-Memory Transformers\n",
    "\n",
    "### 2.1 What Are They?\n",
    "- **Definition**: Enhanced transformers that handle long sequences (thousands of words) by storing and retrieving distant information efficiently, like a brain’s long-term memory.\n",
    "- **Analogy**: Standard transformers are a notepad; long-memory transformers are a library with indexed books for quick recall.\n",
    "- **Logic**: Use hierarchies or external memory to summarize and access past context, reducing complexity to O(n).\n",
    "- **Why for NLG?**: Essential for coherent long-text generation (e.g., novels, dialogues).\n",
    "- **Benefits**: Linear time, better long-range dependencies, no recency bias.\n",
    "\n",
    "### 2.2 Key Architectures\n",
    "We’ll explore three: Long-Range Memory Transformer (LRMT), Hierarchical Memory Transformer (HMT), and Large Memory Model (LM2).\n",
    "\n",
    "#### 2.2.1 Long-Range Memory Transformer (LRMT)\n",
    "- **Theory**: Separates short-range (nearby words) and long-range (distant) processing, creating **memory tokens** to summarize past context. <grok:render type=\"render_inline_citation\"><argument name=\"citation_id\">46</argument></grok:render>\n",
    "- **Logic**: Strengthens long-range gradients by isolating them.\n",
    "- **Architecture**:\n",
    "  - Process input in chunks.\n",
    "  - Create memory tokens via non-causal attention.\n",
    "  - Retrieve via cross-attention to past memories.\n",
    "  - Complexity: O(n).\n",
    "- **Math**:\n",
    "  - Memory token: M = average(token_vectors).\n",
    "  - Cross-attention: `softmax(Q * Mᵀ / √d) * M`.\n",
    "- **Example Calculation**:\n",
    "  - Text: \"The cat sat. Later, the cat jumped.\"\n",
    "  - Chunk 1: \"The cat sat\" → M1 = [0.5, 0.3].\n",
    "  - Chunk 2: \"Later, the cat jumped\" attends to M1.\n",
    "  - Score = dot([1,0], [0.5,0.3])/√2 = 0.35, softmax ≈ 0.67.\n",
    "  - Output weights M1 at 67%.\n",
    "- **Visualization**: Draw two paths: Main (short-range arrows), Memory (long arrows to past boxes).\n",
    "\n",
    "#### 2.2.2 Hierarchical Memory Transformer (HMT)\n",
    "- **Theory**: Mimics brain hierarchy: Sensory (recent), Short-term (summary), Long-term (cached histories). <grok:render type=\"render_inline_citation\"><argument name=\"citation_id\">47</argument></grok:render>\n",
    "- **Logic**: Summarizes segments, searches past memories for relevance.\n",
    "- **Architecture**:\n",
    "  - Divide input into segments (L tokens).\n",
    "  - Summary: H_sum = model(prompt || segment).\n",
    "  - Search: Q = H_sum * Wq, K = Memories * Wk.\n",
    "  - Attention: `softmax(Q * Kᵀ / √d) * Memories`.\n",
    "- **Example Calculation**:\n",
    "  - H_sum = [1,0], Memory1 = [0.5,0.5], d=2.\n",
    "  - Q * Kᵀ = 0.5/√2 ≈ 0.35, softmax ≈ 1.\n",
    "  - Recalls Memory1 fully.\n",
    "- **Visualization**: Pyramid—Bottom: recent tokens, Middle: summary, Top: cached memories.\n",
    "\n",
    "#### 2.2.3 Large Memory Model (LM2)\n",
    "- **Theory**: Uses auxiliary memory with gates (input/forget/output) for dynamic updates. <grok:render type=\"render_inline_citation\"><argument name=\"citation_id\">45</argument></grok:render>\n",
    "- **Logic**: Like LSTM, gates control memory retention.\n",
    "- **Architecture**: Decoder + memory bank, cross-attention retrieves.\n",
    "- **Visualization**: Transformer with a side “memory vault” and gates as doors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Simple HMT-like implementation\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def summarize_segment(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    outputs = model(**inputs).last_hidden_state.mean(dim=1)  # Summary embedding\n",
    "    return outputs\n",
    "\n",
    "# Example\n",
    "text = 'The cat sat on the mat. Later, the cat jumped.'\n",
    "summary = summarize_segment(text)\n",
    "print(f'Summary embedding shape: {summary.shape}')\n",
    "\n",
    "# Visualization: Plot embedding norms\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.bar(range(summary.shape[1]), summary[0].detach().numpy())\n",
    "plt.title('Summary Embedding Features')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Applications\n",
    "- **Story Generation**: LRMT ensures coherence in novels by recalling early plot points.\n",
    "- **QA Systems**: HMT improves PubMedQA by 1% on long medical texts.\n",
    "- **Conversational AI**: LM2 remembers user preferences over long dialogues.\n",
    "- **Scientific Use**: Generate hypotheses from long papers or summarize experiments.\n",
    "\n",
    "## Section 4: Research Directions & Rare Insights\n",
    "- **Insight**: Long-memory reduces recency bias, critical for unbiased scientific NLG.\n",
    "- **Question**: How does hierarchy affect fairness in text generation?\n",
    "- **Rare Gap**: Standard tutorials skip memory’s impact on gradient stability.\n",
    "- **Experiment Idea**: Fine-tune HMT on PG-19 (books dataset) to test coherence.\n",
    "\n",
    "## Section 5: Mini & Major Projects\n",
    "### Mini Project: Attention Visualization\n",
    "- **Task**: Visualize attention for a 5-word sentence.\n",
    "- **Code**:\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5\n",
    "    def forward(self, x):\n",
    "        q = k = v = x\n",
    "        scores = torch.matmul(q, k.transpose(-2,-1)) * self.scale\n",
    "        return nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "x = torch.rand(1, 5, 64)  # 5 words, 64-dim\n",
    "attn = SimpleAttention(64)\n",
    "scores = attn(x).detach().numpy()[0]\n",
    "plt.imshow(scores, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title('Mini Project: Attention Scores')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Major Project: HMT on PG-19\n",
    "- **Task**: Fine-tune a transformer on PG-19 for long-text generation.\n",
    "- **Steps**:\n",
    "  1. Load PG-19 dataset (Hugging Face).\n",
    "  2. Implement HMT with segment summarization.\n",
    "  3. Evaluate coherence using BLEU score.\n",
    "\n",
    "## Section 6: Exercises\n",
    "1. **Basic**: Calculate attention scores for 3 words manually (solution: similar to 1.2).\n",
    "2. **Intermediate**: Modify the HMT code to include two segments.\n",
    "3. **Advanced**: Test memory token impact on a small dataset.\n",
    "\n",
    "## Section 7: Future Directions\n",
    "- Explore sparse attention for efficiency.\n",
    "- Investigate memory-augmented models for multimodal NLG.\n",
    "- Test long-memory on non-English datasets for robustness.\n",
    "\n",
    "## Section 8: What’s Missing in Standard Tutorials\n",
    "- **Gradient Stability**: Long-memory stabilizes training for long texts.\n",
    "- **Scientific Applications**: Most tutorials focus on commercial uses, not hypothesis generation.\n",
    "- **Math Depth**: Full calculations (as above) are rare but critical for researchers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}