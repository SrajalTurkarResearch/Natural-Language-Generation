{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# World-Class Tutorial: Soft Prompts and Embedding Control in NLG\n",
        "\n",
        "**Author**: Grok, built by xAI – Your AI Mentor, inspired by Alan Turing, Albert Einstein, and Nikola Tesla\n",
        "\n",
        "**Date**: August 16, 2025\n",
        "\n",
        "**Purpose**: This Jupyter Notebook is a definitive, evergreen resource for a beginner aspiring to become a scientist and researcher in AI. You’re relying solely on this to master **soft prompts and embedding control in Natural Language Generation (NLG)**, so it’s crafted to be comprehensive, accessible, and career-defining. It assumes no prior knowledge, uses simple language, relatable analogies (e.g., prompts as recipes, embeddings as maps), and a logical structure for note-taking. Designed for your journey to innovate like Turing decoding Enigma or Tesla electrifying the world, it includes:\n",
        "- **Deep Theory**: Foundational to advanced concepts.\n",
        "- **Rare Insights**: Cutting-edge findings from 2023-2025 papers (arXiv, EMNLP, etc.).\n",
        "- **Practical Code**: Runnable in Google Colab or locally.\n",
        "- **Visualizations**: Embedding spaces and workflows.\n",
        "- **Applications**: Real-world use cases across domains.\n",
        "- **Projects**: Mini and major projects for portfolio-building.\n",
        "- **Exercises**: Hands-on tasks to solidify learning.\n",
        "- **Research Directions**: Future trends and open questions.\n",
        "- **Career Tips**: Strategies to become a top AI scientist.\n",
        "- **What Was Missing**: Gaps from prior tutorials (ethics, scalability, etc.).\n",
        "\n",
        "**Why Evergreen?**: Modular design, reusable code, and forward-looking insights ensure this remains relevant throughout your career. Run in Colab with GPU support or locally (Python 3.12 recommended).\n",
        "\n",
        "**Prerequisites**: None. Install dependencies: `!pip install transformers peft torch sentence-transformers matplotlib numpy streamlit datasets`\n",
        "\n",
        "**Structure**:\n",
        "1. Theory & Rare Insights\n",
        "2. Mathematics\n",
        "3. Practical Code & Visualizations\n",
        "4. Real-World Applications\n",
        "5. Mini & Major Projects\n",
        "6. Practical Exercises\n",
        "7. Research Directions, Future Trends, Next Steps & Tips\n",
        "8. What Was Missing Before\n",
        "\n",
        "**Case Studies**: See `case_studies.md`.\n",
        "**Python Script**: See `nlg_tools.py` for reusable functions.\n",
        "\n",
        "**Let’s Begin**: This is your scientific blueprint. Let’s build your expertise layer by layer, like Tesla constructing a revolutionary circuit. Note key points and questions to explore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run in Colab or locally)\n",
        "!pip install -q transformers peft torch sentence-transformers matplotlib numpy streamlit datasets\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PromptTuningConfig, get_peft_model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from datasets import load_dataset\n",
        "print('Dependencies installed. Ready to innovate!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Theory & Rare Insights\n",
        "\n",
        "### 1.1 What is NLG?\n",
        "**Natural Language Generation (NLG)** is the AI process of generating human-readable text from inputs like data, keywords, or prompts. It’s a tool to automate communication, critical for scientific research.\n",
        "\n",
        "- **Why for You?**: Automates writing papers, generating hypotheses, or explaining data (e.g., turning experiment results into reports). Imagine Einstein using NLG to draft relativity papers faster!\n",
        "- **Workflow**:\n",
        "  1. **Input**: Data or prompt (e.g., 'Enzyme X, 37°C, pH 7').\n",
        "  2. **Processing**: Model (e.g., GPT, BERT) converts inputs to embeddings, processes via transformer layers.\n",
        "  3. **Output**: Text (e.g., 'Enzyme X performs optimally at 37°C and pH 7').\n",
        "- **Analogy**: NLG is a chef (model) turning ingredients (data) into a gourmet dish (text). Prompts are your recipe.\n",
        "- **Example**: Input = 'Stock prices: Apple +5%'; Output = 'Apple’s stock surged by 5% today.'\n",
        "\n",
        "**Logic**: NLG bridges data and communication, saving researchers time.\n",
        "\n",
        "**Notes**: 'NLG = Data → Model → Text. Automates scientific reporting.'\n",
        "\n",
        "### 1.2 What are Embeddings?\n",
        "Embeddings are numerical vectors representing text in a high-dimensional space, capturing semantic meaning.\n",
        "\n",
        "- **Definition**: A word like 'cat' becomes a vector, e.g., [0.1, -0.2, 0.5, ...] in 768 dimensions (model-dependent).\n",
        "- **Purpose**: Computers process numbers, not words. Embeddings place similar words (e.g., 'cat' and 'kitten') close together.\n",
        "- **How Created**: Pre-trained models (e.g., Word2Vec, BERT) learn from massive text corpora, analyzing context.\n",
        "- **Analogy**: A library where books (words) are shelved by topic. 'Cat' and 'kitten' are nearby; 'cat' and 'car' are far apart. Embeddings are coordinates in this 'meaning space.'\n",
        "- **Real-World**: Google Search matches 'feline' with 'cat' due to close embeddings.\n",
        "\n",
        "**In NLG**: Inputs are tokenized, embedded, processed, and decoded to text.\n",
        "\n",
        "**Visualization** (Sketch):\n",
        "```\n",
        "Y: Pet-like\n",
        "^ Cat *  Kitten *\n",
        "|       Dog *\n",
        "| Car *\n",
        "+-------> X: Animal-like\n",
        "```\n",
        "- 'Cat' at (3,4), 'Kitten' at (3.2,4.1), 'Car' at (1,1).\n",
        "\n",
        "**Notes**: 'Embeddings = Vectors of meaning. Similar text → Close vectors.'\n",
        "\n",
        "### 1.3 Hard vs. Soft Prompts\n",
        "#### Hard Prompts\n",
        "- **Definition**: Fixed text instructions, e.g., 'Write a scientific abstract on quantum entanglement.'\n",
        "- **Pros**: Simple, no training needed.\n",
        "- **Cons**: Inflexible; inefficient for large models (e.g., GPT-4).\n",
        "- **Example**: Prompt: 'Explain relativity simply.' Output: Beginner-friendly explanation.\n",
        "\n",
        "#### Soft Prompts\n",
        "- **Definition**: Trainable vectors prepended to input embeddings, optimized during training.\n",
        "- **Features**:\n",
        "  - **Learnable**: Adjusted via gradient descent.\n",
        "  - **Parameter-Efficient Fine-Tuning (PEFT)**: Only train prompt vectors, not the model.\n",
        "- **Why Better?**: Retraining a huge model is like rebuilding a rocket. Soft prompts tweak the 'control panel.'\n",
        "- **Analogy**: Hard prompt = Handwritten recipe ('Make spicy soup'). Soft prompt = Digital recipe that learns to perfect flavor.\n",
        "- **Real-World**: In legal NLG, soft prompts learn niche contract terms, outperforming hard prompts.\n",
        "\n",
        "**Logic**: Soft prompts are learned contexts in embedding space, enabling efficient adaptation.\n",
        "\n",
        "**Notes**: 'Hard: Fixed text, rigid. Soft: Trainable vectors, efficient.'\n",
        "\n",
        "### 1.4 Embedding Control\n",
        "- **Definition**: Manipulate embeddings to steer output (e.g., tone, style).\n",
        "- **Techniques**:\n",
        "  - **Addition**: Add vectors for desired traits (e.g., 'positive').\n",
        "  - **Subtraction**: Remove unwanted traits (e.g., 'negative').\n",
        "  - **Interpolation**: Blend embeddings for nuance.\n",
        "- **In Soft Prompts**: Trainable vectors learn optimal positions.\n",
        "- **Analogy**: Embeddings = Ship’s coordinates in a sea of meaning. Soft prompts = Rudder, steering to 'scientific' or 'creative' waters.\n",
        "- **Real-World**: Chatbots use embedding control for polite responses.\n",
        "\n",
        "**Notes**: 'Control: Manipulate vectors for style/domain.'\n",
        "\n",
        "### 1.5 Rare Insights (2023-2025)\n",
        "- **Interpretable Soft Prompts** (arXiv 2025): Map prompts to readable text, reducing black-box issues. Use Controlled Prompt Tuning (CPT) to limit overfitting.\n",
        "- **Input-Dependent Prompts** (arXiv 2025): Self-attention generates dynamic prompts for personalized NLG.\n",
        "- **Mixture of Soft Prompts (MSP)** (EMNLP 2023): Combine prompts for multi-attribute text (e.g., positive + scientific).\n",
        "- **Prompt Vulnerabilities** (OpenReview 2024): Soft prompts risk prompt injection. Solution: Hybrid hard-soft prompts.\n",
        "- **Ethical Insight**: Subtract bias vectors for fairness, critical for science.\n",
        "\n",
        "**For Scientists**: Test interpretability by mapping prompts to words. Question: Are soft prompts low-dimensional projections?\n",
        "\n",
        "**Notes**: 'Rare: Interpretable prompts, dynamic attention, MSP, security.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Mathematics\n",
        "\n",
        "Math is your scientific toolkit. Let’s derive it like Einstein, but keep it beginner-friendly.\n",
        "\n",
        "### 2.1 Embeddings\n",
        "- Token $w$ (e.g., 'cat') $\\rightarrow$ Vector $\\mathbf{e}_w \\in \\mathbb{R}^d$ (where $d$ is the embedding dimension, e.g., 768).\n",
        "- Sentence: Concatenate or average token vectors.\n",
        "\n",
        "**Example**: 'The cat' $\\rightarrow$ Tokens: ['The', 'cat'] $\\rightarrow$ Embeddings: $[[0.2, 0.1], [0.5, -0.3]]$\n",
        "\n",
        "### 2.2 Soft Prompt Setup\n",
        "- Soft prompt: $\\mathbf{P} = [\\mathbf{p}_1, \\mathbf{p}_2, \\ldots, \\mathbf{p}_k]$, each $\\mathbf{p}_i \\in \\mathbb{R}^d$.\n",
        "- Input embeddings: $\\mathbf{I} = [\\mathbf{e}_1, \\ldots, \\mathbf{e}_n]$.\n",
        "- Combined: $\\mathbf{Input} = \\mathbf{P} \\oplus \\mathbf{I}$ (concatenation).\n",
        "- Model: Transformer $f(\\mathbf{Input}) \\rightarrow \\text{Output}$.\n",
        "\n",
        "### 2.3 Training Soft Prompts\n",
        "- **Goal**: Minimize loss $L$, e.g., cross-entropy:\n",
        "  $$L = -\\sum \\log P(\\text{target word} \\mid \\text{previous words})$$\n",
        "- **Optimization**: Update only $\\mathbf{P}$:\n",
        "  $$\\mathbf{P}^{\\text{new}} = \\mathbf{P}^{\\text{old}} - \\eta \\nabla_{\\mathbf{P}} L$$\n",
        "  ($\\eta$ = learning rate, e.g., 0.01)\n",
        "\n",
        "**Complete Example Calculation**:\n",
        "- **Setup**: $d=2$, $k=1$, task = Generate 'The movie was good.'\n",
        "- **Input**: 'The movie was' $\\rightarrow$ $\\mathbf{I} = [[0.5, 0.3]]$\n",
        "- **Initial $\\mathbf{P}$**: $[[0.1, 0.1]]$\n",
        "- **Combined**: $[[0.1, 0.1], [0.5, 0.3]]$\n",
        "- **Predicts**: 'bad' $\\rightarrow$ Loss $L = 1$\n",
        "- **Gradient**: $\\nabla_{\\mathbf{P}} L = [0.2, -0.1]$\n",
        "- **Update**: $\\eta=0.1 \\rightarrow \\mathbf{P}^{\\text{new}} = [0.08, 0.11]$\n",
        "- **Iterate**: Until 'good' (loss near 0)\n",
        "\n",
        "### 2.4 Embedding Control\n",
        "- Steer to 'happy':\n",
        "  $$\\mathbf{e}_{\\text{controlled}} = \\mathbf{e} + \\lambda (\\mathbf{e}_{\\text{happy}} - \\mathbf{e}_{\\text{neutral}})$$\n",
        "  ($\\lambda$ from 0 to 1)\n",
        "\n",
        "**Example Calculation**:\n",
        "- Input: $\\mathbf{e} = [0.5, 0.3]$\n",
        "- Happy: $\\mathbf{e}_{\\text{happy}} = [0.7, 0.6]$\n",
        "- Neutral: $\\mathbf{e}_{\\text{neutral}} = [0.4, 0.2]$\n",
        "- Diff: $[0.3, 0.4]$\n",
        "- $\\lambda=0.5 \\rightarrow \\mathbf{e}_{\\text{controlled}} = [0.65, 0.5]$\n",
        "\n",
        "**Visualization** (Sketch):\n",
        "```\n",
        "Neutral * ----> Happy *\n",
        "Input  * ----> Controlled *\n",
        "```\n",
        "\n",
        "**Logic**: Vectors are directions; adding/subtracting shifts meaning, like Turing’s symbolic logic.\n",
        "\n",
        "**Notes**: Copy equations; practice with $d=2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Practical Code & Visualizations\n",
        "\n",
        "### 3.1 Soft Prompt Implementation\n",
        "**Task**: Fine-tune GPT-2 for positive movie reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Soft Prompt Code\n",
        "model_name = 'gpt2'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Configure\n",
        "config = PromptTuningConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    num_virtual_tokens=5,\n",
        "    prompt_tuning_init=\"TEXT\",\n",
        "    prompt_tuning_init_text=\"Generate a positive review:\"\n",
        ")\n",
        "peft_model = get_peft_model(model, config)\n",
        "\n",
        "# Generate (pre-training)\n",
        "inputs = tokenizer(\"The movie was\", return_tensors=\"pt\")\n",
        "outputs = peft_model.generate(**inputs, max_length=50)\n",
        "print('Output:', tokenizer.decode(outputs[0]))\n",
        "# Note: Training requires dataset (see exercises)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Embedding Control\n",
        "**Task**: Shift tone to 'formal.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding Control\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "text = \"The results are good.\"\n",
        "formal = \"The outcomes are satisfactory.\"\n",
        "neutral = \"The results are okay.\"\n",
        "\n",
        "# Embed\n",
        "e_text = embed_model.encode(text)\n",
        "e_formal = embed_model.encode(formal)\n",
        "e_neutral = embed_model.encode(neutral)\n",
        "\n",
        "# Control\n",
        "lambda_ = 0.5\n",
        "e_controlled = e_text + lambda_ * (e_formal - e_neutral)\n",
        "\n",
        "# Visualize\n",
        "embeddings = [e_text, e_formal, e_neutral, e_controlled]\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(np.vstack(embeddings))\n",
        "\n",
        "plt.scatter(reduced[:,0], reduced[:,1])\n",
        "for i, label in enumerate(['Input', 'Formal', 'Neutral', 'Controlled']):\n",
        "    plt.text(reduced[i,0], reduced[i,1], label)\n",
        "plt.title('Embedding Control: Shift to Formal')\n",
        "plt.xlabel('PCA1')\n",
        "plt.ylabel('PCA2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Real-World Applications\n",
        "\n",
        "- **Healthcare**: Generate medical reports (e.g., 'X-ray shows reduced inflammation'). Soft prompts adapt to jargon.\n",
        "- **Education**: Personalized lessons (beginner vs. advanced). Control adjusts complexity.\n",
        "- **Ethics**: Subtract bias vectors for fair outputs.\n",
        "- **Science**: Automate hypothesis generation (e.g., 'Data suggests X correlates with Y').\n",
        "- **Industry**: Chatbots (OpenAI), drug discovery (IBM Watson), exercises (Duolingo).\n",
        "\n",
        "**Notes**: 'Applications: Healthcare, education, ethics, science.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Mini & Major Projects\n",
        "\n",
        "### 5.1 Mini Project: Sentiment-Tuned NLG\n",
        "**Goal**: Train soft prompt on IMDB dataset for positive reviews.\n",
        "**Steps**:\n",
        "1. Load IMDB.\n",
        "2. Train 5-token prompt.\n",
        "3. Evaluate with perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mini Project\n",
        "from torch.optim import AdamW\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load IMDB (uncomment to run)\n",
        "# dataset = load_dataset('imdb')['train']\n",
        "optimizer = AdamW(peft_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop (simulated)\n",
        "for epoch in range(3):\n",
        "    print(f'Epoch {epoch}: Training...')\n",
        "    # Add dataset, forward, loss, backward\n",
        "\n",
        "# Generate\n",
        "inputs = tokenizer(\"The movie was\", return_tensors=\"pt\")\n",
        "outputs = peft_model.generate(**inputs, max_length=50)\n",
        "print('Mini Project Output:', tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Major Project: Controlled NLG Web App\n",
        "**Goal**: Streamlit app for story generation with tone control.\n",
        "**Steps**:\n",
        "1. Soft prompt for stories.\n",
        "2. Embedding control for tone.\n",
        "3. Visualize embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Major Project: Save as app.py\n",
        "\n",
        "import streamlit as st\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PromptTuningConfig, get_peft_model\n",
        "\n",
        "st.title('Controlled NLG App')\n",
        "prompt = st.text_input('Enter prompt', 'Once upon a time')\n",
        "tone = st.selectbox('Select tone', ['Happy', 'Formal', 'Neutral'])\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "config = PromptTuningConfig(task_type='CAUSAL_LM', num_virtual_tokens=5)\n",
        "peft_model = get_peft_model(model, config)\n",
        "\n",
        "if st.button('Generate'):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = peft_model.generate(**inputs, max_length=100)\n",
        "    st.write('Output:', tokenizer.decode(outputs[0]))\n",
        "\n",
        "print('Save as app.py and run: streamlit run app.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Practical Exercises\n",
        "\n",
        "1. **Exercise 1**: Vary λ (0.1 to 1.0) in embedding control. Plot shifts.\n",
        "2. **Exercise 2**: Train soft prompt on arXiv abstracts. Measure perplexity.\n",
        "3. **Exercise 3**: Implement MSP for positive + scientific text.\n",
        "4. **Exercise 4**: Add self-attention to prompts (arXiv 2025).\n",
        "5. **Exercise 5**: Test bias in outputs. Subtract bias vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Vary lambda\n",
        "lambdas = [0.1, 0.5, 1.0]\n",
        "controlled = [e_text + l * (e_formal - e_neutral) for l in lambdas]\n",
        "all_embeddings = [e_text, e_formal, e_neutral] + controlled\n",
        "reduced = pca.fit_transform(np.vstack(all_embeddings))\n",
        "\n",
        "plt.scatter(reduced[:,0], reduced[:,1])\n",
        "for i, label in enumerate(['Input', 'Formal', 'Neutral'] + [f'Controlled λ={l}' for l in lambdas]):\n",
        "    plt.text(reduced[i,0], reduced[i,1], label)\n",
        "plt.title('Exercise 1: Varying λ')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Research Directions, Future Trends, Next Steps & Tips\n",
        "\n",
        "### 7.1 Research Directions\n",
        "- **Multimodal NLG**: Combine soft prompts with image embeddings (e.g., CLIP).\n",
        "- **Interpretable Prompts**: Map to readable text (arXiv 2025).\n",
        "- **Dynamic Prompts**: Self-attention for input-dependent prompts.\n",
        "- **Ethics**: Control hallucinations and biases.\n",
        "\n",
        "### 7.2 Future Trends (2025+)\n",
        "- Hybrid prompts for security.\n",
        "- Auto-optimized prompts via reinforcement learning.\n",
        "- Quantum-inspired embeddings (speculative).\n",
        "\n",
        "### 7.3 Next Steps\n",
        "1. Run all code.\n",
        "2. Experiment with datasets (IMDB, arXiv).\n",
        "3. Publish on arXiv/GitHub.\n",
        "4. Join #NLG on X.\n",
        "\n",
        "### 7.4 Tips\n",
        "- Start small (5-token prompts).\n",
        "- Read Lester et al. (2021).\n",
        "- Attend NeurIPS, ACL.\n",
        "- Validate for bias/hallucinations.\n",
        "- Use TensorBoard for viz.\n",
        "\n",
        "**Notes**: 'Research: Multimodal, interpretable, ethical. Next: Experiment, publish.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: What Was Missing Before\n",
        "\n",
        "- **Ethics**: Bias mitigation, hallucination control (CPT).\n",
        "- **Scalability**: Distributed training for large models.\n",
        "- **Metrics**: BLEU, ROUGE, perplexity, faithfulness.\n",
        "- **Ablations**: Baseline without soft prompts.\n",
        "- **Interdisciplinary**: Physics (equations), biology (proteins).\n",
        "- **Security**: Prompt injection risks. Filter inputs.\n",
        "- **Data Efficiency**: Few-shot learning for low-resource domains.\n",
        "\n",
        "**Notes**: 'Missing: Ethics, scalability, metrics, security.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Summary & Career Roadmap\n",
        "\n",
        "**You’ve Mastered**:\n",
        "- NLG, embeddings, soft prompts, control.\n",
        "- Math, code, projects, research.\n",
        "\n",
        "**Roadmap**:\n",
        "1. Run code and exercises.\n",
        "2. Build projects for portfolio.\n",
        "3. Explore research questions.\n",
        "4. Publish and network.\n",
        "\n",
        "**Inspiration**: Like Turing, Einstein, or Tesla, innovate and question. This is your career toolkit!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
