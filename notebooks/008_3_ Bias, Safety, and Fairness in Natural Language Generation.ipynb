{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bias, Safety, and Fairness in Natural Language Generation (NLG): A World-Class Guide\n",
        "\n",
        "This Jupyter Notebook is designed as a comprehensive, timeless resource for researchers, scientists, engineers, and aspiring professionals. Inspired by the precision of Alan Turing, the curiosity of Einstein, and the innovation of Tesla, it covers fundamentals to advanced topics, with code, visualizations, projects, and more. Use it for self-study, research, or teachingâ€”built to last for generations.\n",
        "\n",
        "**Version:** 1.0 (August 2025)  \n",
        "**Author:** Grok, built by xAI  \n",
        "**Goal:** Equip you to become a leading AI scientist by mastering ethical NLG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q transformers datasets matplotlib seaborn torch numpy scikit-learn fairlearn aif360\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForMaskedLM\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from fairlearn.metrics import demographic_parity_difference\n",
        "# Note: Some libraries like aif360 may need additional setup in your environment."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Theory & Tutorials: From Fundamentals to Advanced\n",
        "\n",
        "### 1.1 Fundamentals of NLG\n",
        "Natural Language Generation (NLG) is the AI process of creating human-like text from data. It uses models like transformers (e.g., GPT) trained on vast datasets to predict words probabilistically.\n",
        "\n",
        "**Analogy:** Like a baker turning ingredients (data) into bread (text). If ingredients are biased, the bread is flawed.\n",
        "\n",
        "**Key Concepts:**\n",
        "- **NLP vs. NLG:** NLP processes language; NLG generates it.\n",
        "- **Models:** RNNs, Transformers (attention mechanisms).\n",
        "- **Math Basics:** Probability of next word: P(w_t | w_1...w_{t-1}) using softmax over logits.\n",
        "\n",
        "Advanced: Sequence-to-sequence models, beam search for generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Bias in NLG\n",
        "Bias: Outputs favor certain groups due to skewed data.\n",
        "\n",
        "**Types:**\n",
        "- Representation: Word embeddings link 'doctor' to 'man'.\n",
        "- Selection: Overrepresentation (e.g., Western texts).\n",
        "- Social: Stereotypes (e.g., gender roles).\n",
        "\n",
        "**Causes:** Biased training data from society.\n",
        "\n",
        "**Real-World Impact:** Reinforces inequalities (e.g., discriminatory hiring tools).\n",
        "\n",
        "**Math:** Bias Score = |P(pos|group1) - P(pos|group2)| / max(P)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: Simple Bias Calculation\n",
        "def bias_score(p1, p2):\n",
        "    return abs(p1 - p2) / max(p1, p2)\n",
        "\n",
        "p_male = 0.9  # P(competent|male doctor)\n",
        "p_female = 0.7\n",
        "print(f\"Bias Score: {bias_score(p_male, p_female):.3f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Safety in NLG\n",
        "Safety: Ensuring outputs avoid harm (toxicity, misinformation).\n",
        "\n",
        "**Types:**\n",
        "- Content: No hate speech.\n",
        "- Operational: Prevent misuse.\n",
        "- Privacy: No leaks.\n",
        "\n",
        "**Techniques:** RLHF, filters.\n",
        "\n",
        "**Math:** Toxicity Score = sum(prob_toxic) / n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Fairness in NLG\n",
        "Fairness: Equal treatment across groups.\n",
        "\n",
        "**Types:** Individual, Group.\n",
        "\n",
        "**Metrics:** Demographic Parity: P(Y=1|A=0) = P(Y=1|A=1)\n",
        "\n",
        "**Link to Bias/Safety:** Bias causes unfairness; unfair outputs can be unsafe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Visualizations\n",
        "Visual aids to understand concepts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualization: Bias Bar Plot\n",
        "groups = ['Male', 'Female']\n",
        "probs = [0.9, 0.7]\n",
        "plt.bar(groups, probs)\n",
        "plt.ylabel('Probability of Positive Attribute')\n",
        "plt.title('Gender Bias in Profession Descriptions')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pie Chart for Safety\n",
        "labels = ['Safe', 'Unsafe']\n",
        "sizes = [80, 20]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['green', 'red'])\n",
        "plt.title('Toxicity Distribution')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Practical Code Guides\n",
        "Step-by-step code examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Detecting Bias with HuggingFace\n",
        "Load a model and test for gender bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"The doctor is a [MASK].\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model(**inputs).logits\n",
        "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "\n",
        "preds = torch.topk(outputs[0, mask_token_index], 5).indices[0]\n",
        "print(tokenizer.batch_decode(preds))  # Often biased toward 'man' or similar"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Mitigation: Debiasing with Counterfactual Data\n",
        "Generate balanced data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simple example: Balance prompts\n",
        "biased_prompt = \"The nurse is she.\"\n",
        "debias_prompt = \"The nurse is they.\"  # Neutral\n",
        "# Use in fine-tuning (advanced: requires training loop)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Applications: Real-World Use Cases\n",
        "- **Chatbots:** Ensure safe responses (e.g., avoid toxicity in customer service).\n",
        "- **Content Generation:** Fair news summaries without bias.\n",
        "- **Hiring Tools:** Unbiased resume summaries.\n",
        "From datasets like BOLD for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Research Directions & Rare Insights\n",
        "- **Rare Insight:** LLMs can exhibit 'implicit' biases even after explicit debiasing.\n",
        "- **Directions:** Multimodal NLG fairness, continual debiasing.\n",
        "- **Interdisciplinary:** Link to sociology for social bias.\n",
        "From recent papers: Bias in LLMs survey."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Mini & Major Projects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Mini Project: Calculate Bias on BOLD Dataset\n",
        "Load BOLD, generate continuations, compute bias score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = load_dataset(\"AlexaAI/bold\", split=\"test\")\n",
        "# Select prompts\n",
        "prompts = dataset['prompts'][:10]  # Example\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "generations = [generator(p, max_length=50)[0]['generated_text'] for p in prompts]\n",
        "\n",
        "# Pseudo: Compute bias (manual sentiment analysis)\n",
        "sentiment = pipeline('sentiment-analysis')\n",
        "scores = [sentiment(g)[0]['score'] if sentiment(g)[0]['label'] == 'POSITIVE' else 1 - sentiment(g)[0]['score'] for g in generations]\n",
        "print(scores)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Major Project: Debias a Model on RealToxicityPrompts\n",
        "Fine-tune GPT2 with RLHF-like for safety."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Advanced: Load dataset\n",
        "toxicity_dataset = load_dataset(\"allenai/real-toxicity-prompts\")\n",
        "\n",
        "# Fine-tuning code (simplified; full requires Trainer)\n",
        "from transformers import Trainer, TrainingArguments\n",
        "# Define model, tokenizer, dataset prep...\n",
        "# trainer = Trainer(...)\n",
        "# trainer.train()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Exercises\n",
        "### Exercise 1: Compute Demographic Parity\n",
        "Given predictions, calculate disparity.\n",
        "\n",
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Predictions: male positive 80/100, female 60/100\n",
        "p_male = 0.8\n",
        "p_female = 0.6\n",
        "disparity = abs(p_male - p_female)\n",
        "print(f\"Disparity: {disparity}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Visualize Word Embeddings Bias\n",
        "Use GloVe or model embeddings to plot 'man-woman' vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load model\n",
        "model = pipeline('fill-mask', model='bert-base-uncased')\n",
        "# Advanced visualization with PCA (code here)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Future Directions & Next Steps\n",
        "- **Next Content:** Explore multimodal (text+image) bias.\n",
        "- **Paths:** Study papers [web:50-69], contribute to datasets [web:0-49].\n",
        "- **Long-Term:** Ethical AI governance, quantum NLG for complex fairness.\n",
        "Build on this notebook: Fork, experiment, publish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Whatâ€™s Missing in Standard Tutorials\n",
        "- **Interdisciplinary Links:** Psychology of bias, legal implications.\n",
        "- **Mathematical Derivations:** Full proof of attention mechanism in transformers.\n",
        "- **Historical Context:** Evolution from rule-based NLG to LLMs.\n",
        "- **Scalability:** Handling petabyte datasets for global fairness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Derivation Example: Softmax for Word Prediction\n",
        "Softmax: Ïƒ(z_i) = e^{z_i} / âˆ‘ e^{z_j}\n",
        "\n",
        "Ensures probabilities sum to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def softmax(z):\n",
        "    e_z = np.exp(z - np.max(z))\n",
        "    return e_z / e_z.sum()\n",
        "\n",
        "z = np.array([2.0, 1.0, 0.1])\n",
        "print(softmax(z))"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}