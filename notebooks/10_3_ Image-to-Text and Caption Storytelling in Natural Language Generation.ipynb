{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# World-Class Tutorial on Image-to-Text and Caption Storytelling in Natural Language Generation (NLG)\n",
        "\n",
        "## Introduction\n",
        "As an aspiring scientist, researcher, professor, engineer, or mathematician—in the spirit of Alan Turing's computational innovations, Albert Einstein's theoretical insights, and Nikola Tesla's engineering genius—this Jupyter Notebook serves as your comprehensive guide to mastering Image-to-Text (image captioning) and Caption Storytelling in NLG. This notebook is designed to be self-contained, professional, and rigorous, equipping you with the knowledge to advance your career in AI research. It covers fundamentals to advanced topics, practical code, visualizations, applications, projects, exercises, and forward-looking insights.\n",
        "\n",
        "We'll use Python with libraries like Transformers (for models), Matplotlib (for plots), and PIL (for images). Run cells sequentially for best results.\n",
        "\n",
        "**Prerequisites**: Basic Python knowledge. Install dependencies: `pip install transformers torch matplotlib pillow datasets`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Theory & Tutorials – From Fundamentals to Advanced\n",
        "\n",
        "### Fundamentals\n",
        "Image-to-Text (Image Captioning): The task of generating textual descriptions from images, combining Computer Vision (CV) for feature extraction and Natural Language Processing (NLP) for text generation.\n",
        "\n",
        "- **Key Components**:\n",
        "  - Encoder: Extracts image features (e.g., using CNNs like ResNet).\n",
        "  - Decoder: Generates text (e.g., using RNNs/LSTMs or Transformers).\n",
        "  - Attention: Aligns image regions with words.\n",
        "\n",
        "### Advanced Concepts\n",
        "Recent advances (2024-2025) include Multimodal Large Language Models (MLLMs) like LLaVA or PaliGemma, which integrate vision and language for zero-shot captioning. Transformer-based models dominate, with zero-shot capabilities via triggers (e.g., TPCap paper, 2025).\n",
        "\n",
        "Visual Storytelling extends captioning to narratives, incorporating sequence modeling (e.g., BiLSTM in 2025 papers).\n",
        "\n",
        "**Mathematical Foundation**:\n",
        "- Feature Extraction: $f = \\mathrm{CNN}(I)$\n",
        "- Caption Probability: $P(\\mathbf{w} \\mid f) = \\prod P(w_t \\mid \\mathbf{w}_{<t}, f)$\n",
        "- Attention: $\\alpha = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)$, then context $= \\alpha V$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Practical Code Guides\n",
        "\n",
        "### Step-by-Step: Basic Image Captioning with BLIP\n",
        "BLIP (from Salesforce) is a strong baseline model for captioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load model\n",
        "processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n",
        "model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n",
        "\n",
        "# Example image\n",
        "url = 'https://www.example.com/sample_image.jpg'  # Replace with a real URL, e.g., 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Generate caption\n",
        "inputs = processor(image, return_tensors='pt')\n",
        "out = model.generate(**inputs)\n",
        "caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "print('Caption:', caption)\n",
        "\n",
        "# Display image\n",
        "plt.imshow(image)\n",
        "plt.title(caption)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced: Storytelling with Fine-Tuning\n",
        "Extend to storytelling by chaining with a language model like GPT-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load GPT-2 for storytelling\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Generate story from caption\n",
        "prompt = f'Create a short story based on this image description: {caption}. Story:'\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "outputs = gpt_model.generate(**inputs, max_length=100)\n",
        "story = tokenizer.decode(outputs[0])\n",
        "print('Story:', story)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Visualizations\n",
        "\n",
        "### Plotting Attention Maps\n",
        "Visualize how the model attends to image regions (simulated for illustration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulated attention heatmap\n",
        "attention = np.random.rand(224, 224)  # Replace with real attention from model\n",
        "plt.imshow(attention, cmap='hot')\n",
        "plt.title('Attention Heatmap')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Applications – Real-World Examples\n",
        "\n",
        "- **Healthcare**: Medical image captioning (e.g., GPT-based for X-rays, Nature 2023).\n",
        "- **Accessibility**: Microsoft Seeing AI for visually impaired.\n",
        "- **Autonomous Vehicles**: Object description in Tesla Autopilot.\n",
        "- **Industry**: Inventory management in retail via captioning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Research Directions & Rare Insights\n",
        "\n",
        "- **Forward-Looking**: Integrate with MLLMs for video storytelling; address biases in datasets.\n",
        "- **Rare Insights**: Zero-shot captioning via triggers (TPCap, arXiv 2025) reduces training needs; ethical considerations in biased narratives (e.g., cultural insensitivity in global datasets)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Mini & Major Projects\n",
        "\n",
        "### Mini Project: Simple Captioner\n",
        "Build a basic captioner on COCO dataset subset.\n",
        "\n",
        "### Major Project: Storytelling on Flickr30k\n",
        "Fine-tune BLIP + GPT on narrative datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset for project\n",
        "dataset = load_dataset('nlphuji/flickr30k')\n",
        "print(dataset['test'][0])  # Example entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Exercises\n",
        "\n",
        "1. Generate captions for 3 custom images and evaluate manually.\n",
        "\n",
        "**Solution**: Use the code above, compare with ground truth.\n",
        "\n",
        "2. Compute BLEU score for a generated caption.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "reference = [['A', 'cat', 'sleeps', 'on', 'a', 'couch']]\n",
        "candidate = ['A', 'cat', 'is', 'resting']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('BLEU Score:', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Future Directions & Next Steps\n",
        "\n",
        "- Explore PaliGemma or LLaVA for multimodal tasks.\n",
        "- Read key papers: 'Show and Tell' (2015), TPCap (2025).\n",
        "- Join communities: Hugging Face, arXiv for latest research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: What’s Missing in Standard Tutorials\n",
        "\n",
        "- Ethical discussions: Bias in captions (e.g., gender stereotypes).\n",
        "- Scalability: Handling large datasets with distributed training.\n",
        "- Integration with other modalities (e.g., audio for video captioning).\n",
        "- Rare insights: Use of reinforcement learning for diverse captions (e.g., CIDEr optimization)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
