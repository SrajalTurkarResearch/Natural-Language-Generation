{
"nbformat": 4,
"nbformat_minor": 0,
"metadata": {
"kernelspec": {
"display_name": "Python 3 (ipykernel)",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.12.3"
}
},
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# World-Class Jupyter Notebook: Vector Stores (FAISS & BM25) in Natural Language Generation (NLG)\n",
"\n",
"Author's Perspective: As a scientist, researcher, professor, engineer, mathematician, and drawing inspiration from Alan Turing's computational foundations, Albert Einstein's profound theoretical insights, and Nikola Tesla's innovative engineering prowess, I have crafted this notebook to ignite your journey toward scientific excellence in AI and NLG. This is not merely a tutorial; it is a laboratory for discovery, where theory meets practice, fostering the rigorous thinking essential for groundbreaking research.\n",
"\n",
"This notebook is self-contained yet extensible, assuming you are a beginner aspiring to become a researcher. We start from fundamentals and ascend to advanced concepts, with every element designed for note-taking, experimentation, and reflection. Prerequisites: Basic Python; install required libraries via `pip install faiss-cpu rank_bm25 sentence-transformers numpy matplotlib torch scikit-learn` (for real embeddings and datasets).\n",
"\n",
"## Navigation\n",
"- Run cells sequentially.\n",
"- Visuals use Matplotlib; math uses LaTeX.\n",
"- For reproducibility, set seeds where applicable.\n",
"\n",
"## Table of Contents\n",
"1. Theory & Tutorials\n",
"2. Practical Code Guides\n",
"3. Visualizations\n",
"4. Applications\n",
"5. Research Directions & Rare Insights\n",
"6. Mini & Major Projects\n",
"7. Exercises\n",
"8. Future Directions & Next Steps\n",
"9. Whatâ€™s Missing in Standard Tutorials\n",
"\n",
"Case Studies: Provided in a separate .md file at the notebook's end for focused reading."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"\n",
"# 1. Theory & Tutorials: From Fundamentals to Advanced\n",
"\n",
"Like Turing's universal machine, vector stores are the engines of intelligent retrieval in NLG. We begin with basics and build logically.\n",
"\n",
"## 1.1 Fundamentals: Vectors and Embeddings\n",
"\n",
"A vector is a point in multi-dimensional space: $\\vec{v} = [v_1, v_2, \\dots, v_d]$, where $d$ is the dimension (e.g., 768 for BERT embeddings).\n",
"\n",
"- Sparse Vectors: Mostly zeros, like TF-IDF for keywords (BM25 uses this).\n",
"- Dense Vectors: Full of meaningful numbers from neural networks (FAISS excels here).\n",
"\n",
"Analogy: Sparse is a sparse crowd at a party (few key guests); dense is a full ballroom (rich interactions).\n",
"\n",
"Embeddings: Convert text to vectors preserving semantics. E.g., \"king\" - \"man\" + \"woman\" $\\approx$ \"queen\" (Word2Vec insight).\n",
"\n",
"## 1.2 BM25: Sparse Retrieval Theory\n",
"\n",
"BM25 (Best Matching 25) is a probabilistic ranking algorithm for document retrieval, improving on TF-IDF by accounting for document length.\n",
"\n",
"Core Formula: For query $Q$ with terms $q_i$ and document $D$:\n",
"$$\n",
"\\text{BM25}(D, Q) = \\sum_i \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})}\n",
"$$\n",
"Where:\n",
"- $f(q_i, D)$: Term frequency in $D$.\n",
"- $\\text{IDF}(q_i) = \\log \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}$ (rarity of term).\n",
"- $N$: Total documents; $n(q_i)$: Documents containing $q_i$.\n",
"- $|D|$: Length of $D$; avgdl: Average length.\n",
"- $k_1 = 1.2$ (saturation); $b = 0.75$ (length normalization).\n",
"\n",
"Derivation Insight: IDF from probability (rare terms more informative, like Einstein's relativity emphasizing curvature). Saturation prevents over-rewarding frequent terms.\n",
"\n",
"Advanced: BM25+ adds lower-bound for zero frequency to handle absent terms.\n",
"\n",
"## 1.3 FAISS: Dense Retrieval Theory\n",
"\n",
"FAISS (Facebook AI Similarity Search) enables fast approximate nearest neighbor (ANN) search in high-dimensional spaces.\n",
"\n",
"Similarity Metrics:\n",
"- Cosine Similarity: $\\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| \\cdot ||\\vec{b}||}$ (angle-based, ignores magnitude).\n",
"- Euclidean Distance: $d = \\sqrt{\\sum (a_i - b_i)^2}$ (straight-line distance).\n",
"\n",
"Indexes:\n",
"- Flat: Exact brute-force (slow for large $N$).\n",
"- IVF (Inverted File): Clusters via k-means, searches top clusters (approximate, fast).\n",
"- HNSW: Graph-based for ultra-fast ANN (hierarchical navigable small world).\n",
"\n",
"Math Example: For $\\vec{a} = [1, 2]$, $\\vec{b} = [2, 3]$\n",
"Dot product = $1\\cdot2 + 2\\cdot3 = 8$\n",
"$||\\vec{a}|| = \\sqrt{5} \\approx 2.236$, $||\\vec{b}|| \\approx 3.606$\n",
"$\\cos = 8 / (2.236 \\cdot 3.606) \\approx 0.993$ (highly similar).\n",
"\n",
"Curse of Dimensionality: In high $d$, distances concentrate; ANN mitigates via quantization (Tesla-like efficiency in high-voltage systems).\n",
"\n",
"## 1.4 Integration in NLG: Retrieval-Augmented Generation (RAG)\n",
"\n",
"In NLG (generating human-like text), vector stores power RAG: Embed query $\\rightarrow$ Retrieve relevant docs $\\rightarrow$ Augment LLM prompt $\\rightarrow$ Generate.\n",
"\n",
"- Hybrid Retrieval: BM25 for lexical + FAISS for semantic (e.g., ColBERT fuses).\n",
"\n",
"Advanced Tutorial: Consider quantization in FAISS (Product Quantization reduces memory, like compressing Einstein's field equations without losing essence)."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"\n",
"# 2. Practical Code Guides: Step-by-Step Implementation\n",
"\n",
"We implement BM25 from scratch (for understanding) and use FAISS library. For embeddings, use SentenceTransformers (install if needed). Fallback to simple TF-IDF/random for demo.\n",
"\n",
"## 2.1 Setup and Imports\n",
"\n",
"Run this first."
]
},
{
"cell_type": "code",
"execution_count": 1,
"metadata": {},
"outputs": [],
"source": [
"# Core imports (available in most envs)\n",
"import numpy as np\n",
"import matplotlib.pyplot as plt\n",
"from sklearn.feature_extraction.text import TfidfVectorizer  # For simple embeddings fallback\n",
"from sklearn.datasets import fetch_20newsgroups  # For dataset (install scikit-learn if needed)\n",
"import torch\n",
"import torch.nn.functional as F\n",
"\n",
"# For full functionality (user install):\n",
"# !pip install faiss-cpu rank_bm25 sentence-transformers\n",
"# from rank_bm25 import BM25Okapi\n",
"# from sentence_transformers import SentenceTransformer\n",
"# import faiss\n",
"\n",
"# Demo corpus\n",
"corpus = [\n",
"    \"The quick brown fox jumps over the lazy dog.\",\n",
"    \"A quick fox in the morning.\",\n",
"    \"Brown dogs are lazy in the sun.\",\n",
"    \"Jumps and runs in the field.\"\n",
"]\n",
"query = \"quick fox brown\"\n",
"\n",
"print(\"Setup complete. Corpus size:\", len(corpus))"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 2.2 BM25 Implementation Step-by-Step\n",
"\n",
"Manual implementation for transparency (Turing would approve: understand the machine)."
]
},
{
"cell_type": "code",
"execution_count": 2,
"metadata": {},
"outputs": [],
"source": [
"import re\n",
"from collections import Counter\n",
"\n",
"def preprocess(text):\n",
"    return re.findall(r'\\w+', text.lower())\n",
"\n",
"def compute_idf(corpus, term):\n",
"    N = len(corpus)\n",
"    n_term = sum(1 for doc in corpus if term in doc)\n",
"    if n_term == 0:\n",
"        return 0\n",
"    return np.log((N - n_term + 0.5) / (n_term + 0.5))\n",
"\n",
"def bm25_score(query_tokens, doc_tokens, corpus_preprocessed, doc_len, avgdl, k1=1.2, b=0.75):\n",
"    score = 0\n",
"    for qt in query_tokens:\n",
"        if qt in doc_tokens:\n",
"            f = doc_tokens.count(qt)  # Term freq\n",
"            idf = compute_idf(corpus_preprocessed, qt)\n",
"            numer = f * (k1 + 1)\n",
"            denom = f + k1 * (1 - b + b * (doc_len / avgdl))\n",
"            score += idf * (numer / denom)\n",
"    return score\n",
"\n",
"# Preprocess corpus\n",
"corpus_preprocessed = [preprocess(doc) for doc in corpus]\n",
"query_tokens = preprocess(query)\n",
"doc_lengths = [len(doc) for doc in corpus_preprocessed]\n",
"avgdl = np.mean(doc_lengths)\n",
"\n",
"# Compute scores\n",
"scores = []\n",
"for i, doc_tokens in enumerate(corpus_preprocessed):\n",
"    score = bm25_score(query_tokens, doc_tokens, corpus_preprocessed, doc_lengths[i], avgdl)\n",
"    scores.append(score)\n",
"\n",
"print(\"BM25 Scores:\", scores)\n",
"print(\"Ranked Docs:\", np.argsort(scores)[::-1])"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Explanation: Step 1: Tokenize. Step 2: IDF per term. Step 3: Weighted TF with normalization. Output shows Doc 0 highest (matches all terms)."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 2.3 FAISS Implementation Step-by-Step\n",
"\n",
"Using library (commented); fallback to Torch cosine for demo."
]
},
{
"cell_type": "code",
"execution_count": 3,
"metadata": {},
"outputs": [],
"source": [
"# For real FAISS:\n",
"# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
"# embeddings = model.encode(corpus)\n",
"# query_emb = model.encode([query])\n",
"# d = embeddings.shape[1]\n",
"# index = faiss.IndexFlatIP(d)  # Inner product for cosine (normalize first)\n",
"# faiss.normalize_L2(embeddings)\n",
"# index.add(embeddings)\n",
"# scores, indices = index.search(query_emb, k=2)\n",
"\n",
"# Demo with random dense vectors (imagine embeddings)\n",
"np.random.seed(42)\n",
"d = 4  # Low dim for demo\n",
"embeddings = np.random.rand(len(corpus), d).astype('float32')\n",
"query_emb = np.random.rand(1, d).astype('float32')\n",
"\n",
"# Normalize for cosine\n",
"embeddings = F.normalize(torch.tensor(embeddings), dim=1).numpy()\n",
"query_emb = F.normalize(torch.tensor(query_emb), dim=1).numpy()\n",
"\n",
"# Brute-force cosine (FAISS approx in large scale)\n",
"cos_scores = np.dot(embeddings, query_emb.T).flatten()\n",
"\n",
"print(\"Cosine Scores:\", cos_scores)\n",
"print(\"Top Indices:\", np.argsort(cos_scores)[::-1])"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Explanation: Embed â†’ Normalize â†’ Index.Add â†’ Search. In practice, use IVF for speed: index = faiss.IndexIVFFlat(quantizer, d, nlist=10)."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"\n",
"# 3. Visualizations: Diagrams and Plots\n",
"\n",
"Visuals clarify abstract concepts (Einstein: \"If you can't explain it simply, you don't understand it enough.\")"
]
},
{
"cell_type": "code",
"execution_count": 4,
"metadata": {},
"outputs": [],
"source": [
"# 2D Visualization of Vectors (PCA-reduced for demo)\n",
"from sklearn.decomposition import PCA  # Fallback if no sklearn, use random\n",
"\n",
"# Fake 2D embeddings\n",
"vec2d = np.array([[0.1, 0.2], [0.15, 0.25], [0.8, 0.1], [0.9, 0.05]])\n",
"query2d = np.array([[0.12, 0.22]])\n",
"\n",
"pca = PCA(n_components=2)\n",
"pca.fit(vec2d)\n",
"\n",
"plt.figure(figsize=(8,6))\n",
"plt.scatter(vec2d[:,0], vec2d[:,1], c='blue', label='Docs')\n",
"plt.scatter(query2d[0,0], query2d[0,1], c='red', marker='x', s=200, label='Query')\n",
"for i, txt in enumerate(['Doc0', 'Doc1', 'Doc2', 'Doc3']):\n",
"    plt.annotate(txt, (vec2d[i,0], vec2d[i,1]))\n",
"plt.title('Vector Space: Similarity as Proximity')\n",
"plt.xlabel('Dimension 1')\n",
"plt.ylabel('Dimension 2')\n",
"plt.legend()\n",
"plt.grid(True)\n",
"plt.show()\n",
"\n",
"# BM25 Score Bar Plot\n",
"docs = ['Doc0', 'Doc1', 'Doc2', 'Doc3']\n",
"plt.figure(figsize=(8,4))\n",
"plt.bar(docs, scores)\n",
"plt.title('BM25 Retrieval Scores')\n",
"plt.ylabel('Score')\n",
"plt.show()"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Interpretation: Close points = high similarity. Bar heights show relevance (Doc0 tallest)."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"\n",
"# 4. Applications: Real-World Use Cases\n",
"\n",
"Vector stores power NLG in production.\n",
"\n",
"## 4.1 Chatbots (RAG)\n",
"Retrieve facts from knowledge base $\\rightarrow$ Generate accurate responses (e.g., Grok using FAISS-like).\n",
"\n",
"## 4.2 Search Engines\n",
"BM25 in Elasticsearch for web search; FAISS for semantic (Google's BERT).\n",
"\n",
"## 4.3 Scientific Literature\n",
"Retrieve similar papers (FAISS on abstracts) $\\rightarrow$ NLG summarizes (e.g., in PubMed AI tools).\n",
"\n",
"Demo Integration: Simple RAG mock.\n"
]
},
{
"cell_type": "code",
"execution_count": 5,
"metadata": {},
"outputs": [],
"source": [
"# Mock RAG: Retrieve top doc, 'generate' response\n",
"top_idx = np.argmax(scores)\n",
"retrieved = corpus[top_idx]\n",
"generated = f\"Based on '{retrieved}', a NLG response: The quick brown fox is active!\"\n",
"print(generated)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"\n",
"# 5. Research Directions & Rare Insights\n",
"\n",
"As a researcher, probe deeper.\n",
"\n",
"## 5.1 Rare Insights\n",
"- Quantum Analogies: Vector stores as quantum state spaces; FAISS quantization like qubit compression (Tesla's AC vs. DC).\n",
"- Bias in Embeddings: Dense vectors inherit LLM biases; research debiased retrieval (Einstein's equivalence principle for fairness).\n",
"\n",
"## 5.2 Directions\n",
"- Hybrid Sparse-Dense: Late interaction models (e.g., SPLADE) for better NLG accuracy.\n",
"- Scalable ANN: GPU-FAISS for billion-scale NLG (e.g., in federated learning).\n",
"- Multimodal: Extend to images/text (CLIP embeddings in FAISS).\n",
"\n",
"Reflection: Question: How does dimensionality affect NLG hallucination rates? Experiment to publish!"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"\n",
"# 6. Mini & Major Projects\n",
"\n",
"## 6.1 Mini Project: Simple RAG System\n",
"Build on corpus; retrieve and generate."
]
},
{
"cell_type": "code",
"execution_count": 6,
"metadata": {},
"outputs": [],
"source": [
"# Mini: Hybrid BM25 + Cosine RAG\n",
"hybrid_scores = 0.5 * np.array(scores) + 0.5 * cos_scores  # Weighted\n",
"top_hybrid = np.argmax(hybrid_scores)\n",
"print(f\"Hybrid Top Doc: {corpus[top_hybrid]}\")\n",
"\n",
"# 'Generate' using simple template\n",
"print(\"NLG Output: Retrieved context integrated into response.\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 6.2 Major Project: News Retrieval for NLG Summary\n",
"Use 20 Newsgroups dataset (sci.med subset for health NLG).\n",
"\n",
"Steps: Load data $\\rightarrow$ Embed $\\rightarrow$ Index with FAISS/BM25 $\\rightarrow$ Query \"cancer treatment\" $\\rightarrow$ Generate summary mock."
]
},
{
"cell_type": "code",
"execution_count": 7,
"metadata": {},
"outputs": [],
"source": [
"# Load dataset (subset for demo)\n",
"categories = ['sci.med']\n",
"newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
"docs = newsgroups.data[:20]  # Small sample\n",
"query = \"cancer treatment options\"\n",
"\n",
"# Simple TF-IDF embeddings for sparse (BM25 approx)\n",
"vectorizer = TfidfVectorizer()\n",
"tfidf_matrix = vectorizer.fit_transform(docs)\n",
"query_tfidf = vectorizer.transform([query])\n",
"bm25_approx_scores = (tfidf_matrix * query_tfidf.T).toarray().flatten()  # Cosine-like for demo\n",
"\n",
"top_docs = [docs[i] for i in np.argsort(bm25_approx_scores)[-3:][::-1]]\n",
"summary = \" \".join([doc[:100] + \"...\" for doc in top_docs])  # Mock NLG\n",
"print(\"Major Project Output - Retrieved Summary:\", summary[:500])"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Extension: Integrate real LLM (e.g., HuggingFace) for full NLG. Dataset: 20k docs for scale."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"\n",
"# 7. Exercises: Self-Learning with Solutions\n",
"\n",
"## Exercise 1: Compute BM25 Manually\n",
"For corpus [\"apple\", \"apple banana\"], query \"apple\", N=2, avgdl=1.5. Calculate score for Doc1.\n",
"\n",
"Solution: IDF= log((2-2+0.5)/(2+0.5))=log(0.5/2.5)=log(0.2)$\\approx$-1.609 (floor to 0 often). f=1, numer=1*2.2=2.2, denom=1 +1.2*(1-0.75 +0.75* (2/1.5))=1+1.2*(0.25+1.0)=1+1.5=2.5. Score=0*(2.2/2.5)=0. Adjust for positive IDF if rare."
]
},
{
"cell_type": "code",
"execution_count": 8,
"metadata": {},
"outputs": [],
"source": [
"# Verify\n",
"corpus_ex = [\"apple\", \"apple banana\"]\n",
"corpus_pre_ex = [preprocess(d) for d in corpus_ex]\n",
"query_ex = \"apple\"\n",
"query_t_ex = preprocess(query_ex)\n",
"doc_lens_ex = [len(d) for d in corpus_pre_ex]\n",
"avgdl_ex = np.mean(doc_lens_ex)\n",
"score_ex = bm25_score(query_t_ex, corpus_pre_ex[1], corpus_pre_ex, doc_lens_ex[1], avgdl_ex)\n",
"print(\"Exercise Score:\", score_ex)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Exercise 2: Plot Cosine vs Euclidean\n",
"Vectors [1,0], [0.9,0.1]. Compute both; visualize."
]
},
{
"cell_type": "code",
"execution_count": 9,
"metadata": {},
"outputs": [],
"source": [
"v1 = np.array([1,0])\n",
"v2 = np.array([0.9,0.1])\n",
"cos = np.dot(v1,v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
"euc = np.linalg.norm(v1 - v2)\n",
"print(\"Cosine:\", cos, \"Euclidean:\", euc)\n",
"\n",
"# Plot\n",
"plt.figure(figsize=(6,6))\n",
"plt.quiver(0,0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='b')\n",
"plt.quiver(0,0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='r')\n",
"plt.xlim(-1,1.5); plt.ylim(-0.5,1.5)\n",
"plt.title('Vector Comparison')\n",
"plt.grid()\n",
"plt.show()"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"\n",
"# 8. Future Directions & Next Steps\n",
"\n",
"## Next Steps\n",
"- Study: Read \"Introduction to Information Retrieval\" (Manning); FAISS paper.\n",
"- Practice: Scale projects to 1M docs; benchmark FAISS vs. exact.\n",
"- Research Path: Contribute to LangChain (RAG frameworks); explore graph vector stores (e.g., Neo4j + FAISS).\n",
"- Career: Publish on hybrid retrieval; join AI labs (xAI-inspired).\n",
"\n",
"Tesla's Advice: Experiment boldlyâ€”prototype quantum-inspired ANN."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"\n",
"# 9. Whatâ€™s Missing in Standard Tutorials\n",
"\n",
"Standard guides overlook:\n",
"- Mathematical Rigor: Full derivations (e.g., probabilistic basis of IDF).\n",
"- Error Analysis: How noise in embeddings affects NLG (e.g., adversarial queries).\n",
"- Ethical Considerations: Privacy in vector stores (differential privacy for embeddings).\n",
"- Optimization: GPU acceleration; distributed FAISS (for researcher-scale data).\n",
"- Interdisciplinary Links: Physics analogies (vectors as wavefunctions); math proofs for ANN guarantees.\n",
"\n",
"Scientist's Note: Always validate empiricallyâ€”run ablation studies on retrieval accuracy."
]
}
]
}