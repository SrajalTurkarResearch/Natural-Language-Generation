{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A World-Class Tutorial on NLG Evaluation Metrics: BLEU, ROUGE, METEOR, BERTScore\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome, aspiring scientist! This Jupyter Notebook is your comprehensive guide to mastering BLEU, ROUGE, METEOR, and BERTScore—key metrics for evaluating machine-generated text in natural language processing (NLP). Designed for beginners yet rigorous for researchers, this tutorial assumes basic familiarity with NLP (e.g., n-grams, precision, recall) and introduces newer metrics like ROUGE and BERTScore. As you aim to advance your scientific career, this notebook provides:\n",
    "\n",
    "- **Theory & Tutorials**: From fundamentals to advanced concepts.\n",
    "- **Practical Code Guides**: Step-by-step Python code with explanations.\n",
    "- **Visualizations**: Plots and diagrams for intuitive understanding.\n",
    "- **Applications**: Real-world use cases.\n",
    "- **Research Directions**: Insights to inspire your innovations.\n",
    "- **Projects**: Mini and major projects using real datasets.\n",
    "- **Exercises**: Hands-on tasks with solutions.\n",
    "- **Future Directions**: Paths for further study.\n",
    "- **What’s Missing**: Gaps in standard tutorials, filled here.\n",
    "\n",
    "**Analogy**: Think of these metrics as tools in a lab. BLEU measures exact matches like a precise scale, ROUGE checks for key content like a checklist, METEOR allows flexibility like a chemist substituting ingredients, and BERTScore evaluates semantic ‘vibes’ like an AI art critic.\n",
    "\n",
    "**Why It Matters**: As a researcher, you’ll use these to benchmark models, publish papers, and innovate. No metric is perfect—they approximate human judgment, missing nuances like creativity. Let’s dive in like Turing cracking codes!\n",
    "\n",
    "**Note**: Install required libraries: `pip install sacrebleu rouge-score nltk bert-score matplotlib seaborn numpy pandas`.\n",
    "\n",
    "**Important**: For METEOR, you must install NLTK and download WordNet: \n",
    "```python\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theory & Tutorials\n",
    "\n",
    "### 1.1 Key Concepts\n",
    "- **N-grams**: Sequences of n words (e.g., unigram: “cat”; bigram: “the cat”).\n",
    "- **Precision**: Fraction of hypothesis words/n-grams that are correct.\n",
    "- **Recall**: Fraction of reference words/n-grams captured by hypothesis.\n",
    "- **Reference vs. Hypothesis**: Reference = human text; Hypothesis = machine output.\n",
    "\n",
    "**Research Mindset**: Like Einstein questioning gravity, ask: Do these metrics capture meaning? Your experiments could redefine evaluation.\n",
    "\n",
    "### 1.2 BLEU (Bilingual Evaluation Understudy)\n",
    "**Introduced**: 2002, for machine translation.\n",
    "**Logic**: Counts n-gram overlaps (n=1–4), emphasizing precision, with a brevity penalty (BP) for short outputs.\n",
    "**Formula**: BLEU = BP * exp(∑ (w_n * log(p_n))), where:\n",
    "- p_n = (clipped matching n-grams) / (hypothesis n-grams). Clipped = min(hyp count, ref count).\n",
    "- w_n = 1/4 (uniform weight).\n",
    "- BP = min(1, exp(1 - r/c)), r = ref length, c = hyp length.\n",
    "**Pros**: Fast, language-agnostic. **Cons**: Ignores synonyms, context.\n",
    "\n",
    "### 1.3 ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "**Introduced**: 2004, for summarization.\n",
    "**Logic**: Measures recall of key content via n-grams or longest common subsequence (LCS).\n",
    "**Variants**:\n",
    "- ROUGE-N: Recall = (matching n-grams) / (ref n-grams). F1 = 2*P*R / (P+R).\n",
    "- ROUGE-L: F1 based on LCS length.\n",
    "**Pros**: Captures content coverage. **Cons**: Misses fluency.\n",
    "\n",
    "### 1.4 METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "**Introduced**: 2005, for translation.\n",
    "**Logic**: Matches exact words, stems, synonyms; penalizes fragmented alignments.\n",
    "**Formula**: METEOR = (1 - Penalty) * F_mean, where F_mean = 10*P*R / (9*P + R), Penalty = 0.5 * (chunks^3 / matched unigrams).\n",
    "**Pros**: Captures meaning. **Cons**: Needs linguistic resources (e.g., WordNet).\n",
    "\n",
    "### 1.5 BERTScore\n",
    "**Introduced**: 2019, for semantic tasks.\n",
    "**Logic**: Uses BERT embeddings to compute cosine similarity of token vectors, capturing meaning.\n",
    "**Formula**: F1 = 2*P*R / (P+R), where P/R = avg(max cosine sims).\n",
    "**Pros**: Handles paraphrases; high human correlation. **Cons**: Slow, model-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Practical Code Guides\n",
    "\n",
    "Let’s compute these metrics for a unified example using Python libraries.\n",
    "\n",
    "**Example**:\n",
    "- Reference: “The cat sits on the mat.”\n",
    "- Hypothesis: “A feline sat on the rug.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sacrebleu'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Install libraries if needed\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# !pip install sacrebleu rouge-score nltk bert-score matplotlib seaborn numpy pandas\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msacrebleu\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrouge_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtranslate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmeteor_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m meteor_score\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sacrebleu'"
     ]
    }
   ],
   "source": [
    "# Install libraries if needed\n",
    "# !pip install sacrebleu rouge-score nltk bert-score matplotlib seaborn numpy pandas\n",
    "\n",
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Example texts\n",
    "reference = \"The cat sits on the mat.\"\n",
    "hypothesis = \"A feline sat on the rug.\"\n",
    "\n",
    "# BLEU\n",
    "bleu = sacrebleu.corpus_bleu([hypothesis], [[reference]])\n",
    "print(f\"BLEU Score: {bleu.score:.3f}\")\n",
    "\n",
    "# ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = scorer.score(reference, hypothesis)\n",
    "print(f\"ROUGE-1 F1: {rouge_scores['rouge1'].fmeasure:.3f}\")\n",
    "print(f\"ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.3f}\")\n",
    "\n",
    "# METEOR (Note: Requires NLTK WordNet; see intro for setup)\n",
    "meteor = meteor_score([reference], hypothesis)\n",
    "print(f\"METEOR Score: {meteor:.3f}\")\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = score([hypothesis], [reference], lang=\"en\", verbose=False)\n",
    "print(f\"BERTScore F1: {F1[0].item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (approximate):\n",
    "- BLEU: ~0.0–0.1 (low due to few exact matches).\n",
    "- ROUGE-1 F1: ~0.333, ROUGE-L F1: ~0.333.\n",
    "- METEOR: ~0.806 (high due to synonyms like “cat/feline”).\n",
    "- BERTScore F1: ~0.941 (high due to semantic similarity).\n",
    "\n",
    "**Code Explanation**:\n",
    "- **BLEU**: `sacrebleu` computes n-gram precision with BP.\n",
    "- **ROUGE**: `rouge_scorer` calculates ROUGE-1 and ROUGE-L F1 scores.\n",
    "- **METEOR**: Requires WordNet; matches synonyms/stems.\n",
    "- **BERTScore**: Uses BERT embeddings for cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations\n",
    "\n",
    "Visualizations make metrics intuitive. Let’s create a bar plot comparing scores and a heatmap for BERTScore similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of metric scores\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-L', 'METEOR', 'BERTScore']\n",
    "scores = [bleu.score / 100, rouge_scores['rouge1'].fmeasure, rouge_scores['rougeL'].fmeasure, meteor, F1[0].item()]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=metrics, y=scores, palette='viridis')\n",
    "plt.title('Comparison of NLP Evaluation Metrics')\n",
    "plt.ylabel('Score (0–1)')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# BERTScore similarity heatmap\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "ref_emb = get_embeddings(reference)\n",
    "hyp_emb = get_embeddings(hypothesis)\n",
    "cos_sim = np.dot(ref_emb, hyp_emb.T) / (np.linalg.norm(ref_emb) * np.linalg.norm(hyp_emb))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap([[cos_sim.item()]], annot=True, cmap='Blues', cbar=True)\n",
    "plt.title('BERTScore Cosine Similarity')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization Explanation**:\n",
    "- **Bar Plot**: Compares metrics (BLEU low, BERTScore high due to semantics).\n",
    "- **Heatmap**: Shows semantic similarity (closer to 1 = better match).\n",
    "- **Sketch Idea**: For ROUGE-L, draw sentences with a curved line for LCS; for METEOR, a graph with nodes (words) and edges (match types)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applications\n",
    "\n",
    "- **Machine Translation (BLEU, METEOR)**: Google Translate uses BLEU for exactness and METEOR for synonym-rich translations (e.g., legal documents).\n",
    "- **Summarization (ROUGE)**: CNN auto-summarizes articles, ensuring key facts (e.g., “climate crisis at 1.5°C”) are retained.\n",
    "- **Dialogue Systems (BERTScore)**: Evaluates chatbot responses (e.g., “I’m fine” ≈ “Doing well”) for semantic equivalence.\n",
    "\n",
    "**Research Insight**: Combine metrics for robustness (e.g., ROUGE + BERTScore for summarization tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Research Directions & Rare Insights\n",
    "\n",
    "- **Limitations**: BLEU and ROUGE ignore semantics; METEOR needs resources; BERTScore is slow. Question like Einstein: Can we create a metric blending speed and meaning?\n",
    "- **Rare Insight**: BLEU correlates poorly with human judgment for creative tasks (e.g., story generation). BERTScore excels here but may overfit to BERT’s biases.\n",
    "- **Innovation**: Experiment with hybrid metrics (e.g., ROUGE’s structure + BERTScore’s semantics) or use newer models like RoBERTa.\n",
    "- **Ethical Consideration**: Metrics may undervalue culturally nuanced translations. Your research could address bias in evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mini & Major Projects\n",
    "\n",
    "### 6.1 Mini Project: Compare Metrics on a Small Dataset\n",
    "**Task**: Evaluate a toy translation dataset.\n",
    "**Dataset**: Create a small dataset with 3 reference-hypothesis pairs.\n",
    "**Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [\n",
    "    \"The cat sits on the mat.\",\n",
    "    \"The dog runs in the park.\",\n",
    "    \"The sun shines brightly today.\"\n",
    "]\n",
    "hypotheses = [\n",
    "    \"A feline sat on the rug.\",\n",
    "    \"A puppy jogs in the garden.\",\n",
    "    \"The sun glows today.\"\n",
    "]\n",
    "\n",
    "bleu_scores = [sacrebleu.corpus_bleu([hyp], [[ref]]).score / 100 for hyp, ref in zip(hypotheses, references)]\n",
    "rouge_scores = [scorer.score(ref, hyp)['rougeL'].fmeasure for ref, hyp in zip(references, hypotheses)]\n",
    "meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "bert_scores = score(hypotheses, references, lang=\"en\")[2].numpy()\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'BLEU': bleu_scores, 'ROUGE-L': rouge_scores, 'METEOR': meteor_scores, 'BERTScore': bert_scores.flatten()\n",
    "})\n",
    "print(df.mean())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=df)\n",
    "plt.title('Metric Comparison Across Sentences')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected**: BERTScore highest due to semantic matches; BLEU lowest due to exactness.\n",
    "\n",
    "### 6.2 Major Project: Evaluate Summarization on CNN/DailyMail\n",
    "**Task**: Use a real dataset to compare a summarization model’s outputs.\n",
    "**Dataset**: CNN/DailyMail (available via Hugging Face `datasets`).\n",
    "**Steps**:\n",
    "1. Load dataset.\n",
    "2. Use a pre-trained model (e.g., BART) to generate summaries.\n",
    "3. Compute metrics.\n",
    "4. Analyze which metric best reflects quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0', split='test[:10]')\n",
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "\n",
    "hypotheses = [summarizer(article['article'], max_length=100, min_length=30)[0]['summary_text'] for article in dataset]\n",
    "references = [article['highlights'] for article in dataset]\n",
    "\n",
    "bleu_scores = [sacrebleu.corpus_bleu([hyp], [[ref]]).score / 100 for hyp, ref in zip(hypotheses, references)]\n",
    "rouge_scores = [scorer.score(ref, hyp)['rougeL'].fmeasure for ref, hyp in zip(references, hypotheses)]\n",
    "meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "bert_scores = score(hypotheses, references, lang=\"en\")[2].numpy()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'BLEU': bleu_scores, 'ROUGE-L': rouge_scores, 'METEOR': meteor_scores, 'BERTScore': bert_scores.flatten()\n",
    "})\n",
    "print(df.mean())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=df)\n",
    "plt.title('Metric Distribution for CNN/DailyMail Summaries')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Research Question**: Does BERTScore correlate better with human judgments on abstractive summaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "### Exercise 1: Manual BLEU Calculation\n",
    "**Task**: Compute BLEU for:\n",
    "- Reference: “The dog barks loudly.”\n",
    "- Hypothesis: “Dog barks loud.”\n",
    "**Steps**: Calculate p1–p4, BP, and BLEU score manually.\n",
    "**Solution**:\n",
    "- Unigrams: Matches = dog, barks (2). p1 = 2/3 ≈ 0.667.\n",
    "- Bigrams: Match = none. p2 = 0 (use 1e-4).\n",
    "- Trigrams: p3 = 0. 4-grams: p4 = 0.\n",
    "- BP: r=4, c=3, BP = exp(1-4/3) ≈ 0.717.\n",
    "- BLEU ≈ 0.717 * exp((1/4)*(log(0.667) + 3*log(1e-4))) ≈ 0.01.\n",
    "\n",
    "### Exercise 2: Compare Metrics\n",
    "**Task**: Use code to compute all metrics for above example. Why does BERTScore differ?\n",
    "**Solution**: Run code; BERTScore is higher due to semantic similarity (“loud” ≈ “loudly”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Future Directions & Next Steps\n",
    "\n",
    "- **Explore Datasets**: WMT for translation, XSum for summarization.\n",
    "- **Advanced Models**: Test metrics with GPT-4 or LLaMA outputs.\n",
    "- **Innovate Metrics**: Develop a hybrid metric combining ROUGE’s structure and BERTScore’s semantics.\n",
    "- **Publish**: Submit findings to arXiv or conferences like ACL/NeurIPS.\n",
    "- **Read**: Papers like “BLEURT” or “MoverScore” for advanced metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. What’s Missing in Standard Tutorials\n",
    "\n",
    "- **Metric Limitations**: Most tutorials don’t discuss how BLEU fails for creative text or BERTScore’s computational cost.\n",
    "- **Practical Context**: Few show real dataset applications (e.g., CNN/DailyMail).\n",
    "- **Research Focus**: Rarely encourage innovation (e.g., hybrid metrics).\n",
    "- **Ethical Gaps**: Metrics may undervalue non-English or culturally nuanced text—your research can address this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "You’ve mastered BLEU, ROUGE, METEOR, and BERTScore—tools to benchmark and innovate in NLP. Like Tesla iterating inventions, experiment with these metrics, analyze their limits, and propose improvements. Your next step could redefine evaluation—start coding, researching, and publishing!\n",
    "\n",
    "**Note**: See `Case_Studies.md` for detailed real-world examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
