{
"nbformat": 4,
"nbformat_minor": 0,
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.8.5"
}
},
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Mastering Top-k and Top-p Sampling in NLG: A Comprehensive IPython Notebook",
"",
"Prepared by Grok, embodying Alan Turing, Albert Einstein, and Nikola Tesla. This notebook is your gateway to becoming a pioneering AI researcher."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 1: Theory and Tutorial",
"",
"Here we recap and expand on the theory from the previous tutorial. As a beginner scientist, grasp the foundations before coding."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### 1.1 Basics of NLG and Sampling",
"Natural Language Generation (NLG) involves AI models predicting tokens based on probabilities. Sampling techniques like Top-k and Top-p introduce controlled randomness for creativity.",
"",
"Analogy: Like choosing paths in a forest - greedy takes the widest, sampling explores promising ones."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### 1.2 Mathematical Foundations",
"For a vocabulary with logits $z_i$, probabilities via softmax:",
"$\\displaystyle p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$",
"Top-k: Select top k, renormalize.",
"Top-p: Select minimal set where cumulative prob $\\geq p$."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### 1.3 Example Calculation",
"Probs: apple=0.6, banana=0.3, cat=0.1",
"Top-k=2: Renorm apple=0.67, banana=0.33",
"Top-p=0.8: apple + banana =0.9 >0.8, same."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 2: Practical Code Guides",
"",
"Let's implement in Python using NumPy for simplicity. Later, we'll use Torch for real models."
]
},
{
"cell_type": "code",
"metadata": {},
"source": [
"import numpy as np",
"",
"def softmax(x):",
"    e_x = np.exp(x - np.max(x))",
"    return e_x / e_x.sum()",
"",
"def top_k_sampling(logits, k):",
"    probs = softmax(logits)",
"    indices = np.argsort(probs)[::-1]",
"    top_k_indices = indices[:k]",
"    top_k_probs = probs[top_k_indices]",
"    top_k_probs = top_k_probs / top_k_probs.sum()",
"    sampled_index = np.random.choice(top_k_indices, p=top_k_probs)",
"    return sampled_index",
"",
"# Example",
"logits = np.array([3, 2, 1, 0, -1])",
"print(top_k_sampling(logits, 3))"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Top-p Implementation"
]
},
{
"cell_type": "code",
"metadata": {},
"source": [
"def top_p_sampling(logits, p):",
"    probs = softmax(logits)",
"    sorted_indices = np.argsort(probs)[::-1]",
"    sorted_probs = probs[sorted_indices]",
"    cumulative_probs = np.cumsum(sorted_probs)",
"    cutoff_index = np.where(cumulative_probs >= p)[0][0] + 1",
"    nucleus_indices = sorted_indices[:cutoff_index]",
"    nucleus_probs = sorted_probs[:cutoff_index]",
"    nucleus_probs = nucleus_probs / nucleus_probs.sum()",
"    sampled_index = np.random.choice(nucleus_indices, p=nucleus_probs)",
"    return sampled_index",
"",
"print(top_p_sampling(logits, 0.9))"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 3: Visualizations",
"",
"Visualize probability distributions."
]
},
{
"cell_type": "code",
"metadata": {},
"source": [
"import matplotlib.pyplot as plt",
"",
"probs = softmax(logits)",
"tokens = ['apple', 'banana', 'cat', 'dog', 'elephant']",
"",
"plt.bar(tokens, probs)",
"plt.title('Probability Distribution')",
"plt.show()"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Visualizing Top-k"
]
},
{
"cell_type": "code",
"metadata": {},
"source": [
"k = 3",
"indices = np.argsort(probs)[::-1][:k]",
"top_k_probs = probs[indices] / probs[indices].sum()",
"",
"plt.bar([tokens[i] for i in indices], top_k_probs)",
"plt.title('Top-k Renormalized Probs')",
"plt.show()"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 4: Applications",
"",
"- Chatbots: Diverse responses.",
"- Story Generation: Creative plots.",
"- Scientific: Hypothesis generation in research papers."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 5: Mini Projects",
"",
"Mini: Simple Text Generator",
"Use top-k to generate sentences."
]
},
{
"cell_type": "code",
"metadata": {},
"source": [
"# Simple dummy model",
"vocab = {0:'The', 1:'cat', 2:'sat', 3:'on', 4:'mat'}",
"def dummy_logits(prev):",
"    return np.random.randn(5)  # Random for demo",
"",
"def generate_text(sampling_fn, param, length=10):",
"    text = [0]  # Start with 'The'",
"    for _ in range(length):",
"        logits = dummy_logits(text[-1])",
"        next_token = sampling_fn(logits, param)",
"        text.append(next_token)",
"    return ' '.join([vocab[t] for t in text])",
"",
"print(generate_text(top_k_sampling, 3))"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Major Project: Integrate with HuggingFace Transformers",
"Use GPT-2 with top-p sampling for real NLG.",
"",
"Note: In practice, install transformers, but here assume available."
]
},
{
"cell_type": "code",
"metadata": {},
"source": [
"# Pseudocode - run in your env with transformers",
"from transformers import GPT2LMHeadModel, GPT2Tokenizer",
"import torch",
"",
"model = GPT2LMHeadModel.from_pretrained('gpt2')",
"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')",
"",
"def generate_with_top_p(prompt, p=0.9, max_length=50):",
"    inputs = tokenizer.encode(prompt, return_tensors='pt')",
"    outputs = model.generate(inputs, max_length=max_length, do_sample=True, top_p=p)",
"    return tokenizer.decode(outputs[0])",
"",
"print(generate_with_top_p('In a world where AI rules,'))"
],
"execution_count": null,
"outputs": []
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 6: Major Projects on Real-World Examples",
"",
"Project: AI Story Writer",
"Build a system using top-k/top-p to generate stories based on user prompts. Evaluate coherence.",
"",
"Real-World: Medical Report Generation",
"Use to generate varied patient reports from data, ensuring diversity in phrasing."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 7: Research Directions",
"",
"- Combine with reinforcement learning for better control.",
"- Adaptive k/p based on context.",
"- Evaluate on benchmarks like perplexity vs. diversity."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 8: Rare Insights",
"",
"Top-p was introduced in 2019 by Holtzman et al., outperforming top-k in human evaluations due to adaptivity. Insight: In multimodal models, apply similar to image captioning for varied descriptions."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 9: Future Directions & Next Steps",
"",
"- Explore in large models like GPT-4.",
"- Next: Study temperature scaling, beam search hybrids.",
"- Steps: Implement in PyTorch, test on datasets like WikiText."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 10: Tips",
"",
"- Start with small k/p for safety.",
"- Use seed for reproducibility.",
"- Monitor for biases in sampling."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 11: What We Didn't Include (Necessary for Scientists)",
"",
"- Detailed error analysis: When sampling fails (e.g., repetition loops).",
"- Mathematical proofs of convergence.",
"- Integration with RLHF (Reinforcement Learning from Human Feedback).",
"Study papers like 'The Curious Case of Neural Text Degeneration'."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Section 12: Case Studies",
"",
"For separate .md files, copy these sections:",
"",
"### Case Study 1: OpenAI's GPT Models",
"In GPT-3, top-p is default for creativity. Case: Improved story quality over greedy.",
"",
"Save as case_study1.md"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Case Study 2: Google Bard",
"Uses variants for conversational AI. Insight: Reduced hallucinations via controlled sampling.",
"",
"Save as case_study2.md"
]
}
]
}