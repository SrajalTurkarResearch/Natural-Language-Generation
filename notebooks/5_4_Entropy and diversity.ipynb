{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entropy_and_Diversity_in_NLG.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entropy and Diversity in Natural Language Generation (NLG)\n\n",
        "This notebook serves as a comprehensive guide for aspiring scientists, researchers, professors, engineers, and mathematicians—in the spirit of Alan Turing, Albert Einstein, and Nikola Tesla. We'll blend theory, practical code, visualizations, applications, projects, research directions, rare insights, tips, and more. Built upon the foundational tutorial, this expands with hands-on elements.\n\n",
        "## Introduction\n",
        "Recall: Entropy measures uncertainty; diversity ensures variety in NLG outputs. As Turing might say, it's about making machines think diversely like humans.\n\n",
        "We'll use Python with numpy and matplotlib for demos. For NLG, we'll simulate with simple models since no advanced libraries like transformers are pre-installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Theory Recap\n",
        "Shannon Entropy: $ H = - \\sum p_i \\log_2 p_i $\n\n",
        "In NLG: High entropy → diverse, unpredictable text.\n\n",
        "Diversity: Lexical (words), Syntactic (structure), Semantic (meaning).\n\n",
        "Rare Insight: Entropy isn't just randomness; in quantum mechanics (Einstein's domain), it's linked to information in black holes—apply analogously to 'information loss' in repetitive NLG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 1,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def shannon_entropy(probs):\n",
        "    probs = np.array(probs, dtype=np.float64)\n",
        "    probs = probs[probs > 0]\n",
        "    return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "# Example: Fair coin\n",
        "print('Fair coin entropy:', shannon_entropy([0.5, 0.5]))  # 1.0\n",
        "# Biased\n",
        "print('Biased coin:', shannon_entropy([0.99, 0.01]))  # ~0.08"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Visualizations\n",
        "Visualize entropy distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 2,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Low entropy plot\n",
        "probs_low = [0.99, 0.01]\n",
        "plt.bar(['A', 'B'], probs_low)\n",
        "plt.title('Low Entropy')\n",
        "plt.ylabel('Probability')\n",
        "plt.show()\n",
        "\n",
        "# High entropy\n",
        "probs_high = [0.25] * 4\n",
        "plt.bar(['A', 'B', 'C', 'D'], probs_high)\n",
        "plt.title('High Entropy')\n",
        "plt.ylabel('Probability')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Practical Code Guides\n",
        "Guide: Computing word entropy in text.\n\n",
        "Use hardcoded text for demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 3,
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "text = 'The cat sat on the mat. The cat is cute.'\n",
        "words = text.lower().replace('.', '').split()\n",
        "counts = Counter(words)\n",
        "total = len(words)\n",
        "probs = [count / total for count in counts.values()]\n",
        "print('Word Entropy:', shannon_entropy(probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Applications\n",
        "Application: In chatbots, high diversity prevents repetitive responses.\n\n",
        "Real-world: Google Dialogflow uses entropy-like metrics for response variety.\n\n",
        "Code: Simulate diverse generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 4,
      "outputs": [],
      "source": [
        "# Simple Markov chain for text generation\n",
        "transitions = {'the': ['cat', 'dog'], 'cat': ['sat', 'ran'], 'dog': ['barked'], 'sat': ['on'], 'ran': ['away'], 'on': ['the'], 'barked': ['.'], 'away': ['.']}\n",
        "def generate_sentence(start='the', length=5):\n",
        "    sentence = [start]\n",
        "    for _ in range(length-1):\n",
        "        next_words = transitions.get(sentence[-1], ['.'])\n",
        "        next_word = np.random.choice(next_words)\n",
        "        sentence.append(next_word)\n",
        "        if next_word == '.':\n",
        "            break\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "print(generate_sentence())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Mini Project - Entropy Calculator\n",
        "Mini Project: Build a function to compute and visualize text entropy.\n\n",
        "Real-world example: Analyze news articles for bias (low entropy = repetitive phrasing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 5,
      "outputs": [],
      "source": [
        "def visualize_entropy(text):\n",
        "    words = text.lower().replace('.', '').split()\n",
        "    counts = Counter(words)\n",
        "    probs = [count / len(words) for count in counts.values()]\n",
        "    entropy = shannon_entropy(probs)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(list(counts.keys()), list(counts.values()))\n",
        "    plt.title(f'Word Frequency - Entropy: {entropy:.2f}')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xlabel('Word')\n",
        "    plt.show()\n",
        "\n",
        "visualize_entropy('Repeat repeat repeat word word.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Major Project - Diverse Text Generator\n",
        "Major Project: Build a simple NLG system using torch for a basic RNN, measure diversity.\n\n",
        "Real-world: Generate diverse product descriptions for e-commerce.\n\n",
        "Note: Simplified due to environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 6,
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Simple RNN stub (expand in practice)\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(10, 20)\n",
        "\n",
        "# Placeholder: Train on text, generate, compute entropy on outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Research Directions\n",
        "Directions: Explore entropy in multimodal NLG (text+image). Rare Insight: Turing's morphogenesis parallels diversity in language evolution.\n\n",
        "Future: Quantum computing for hyper-diverse generation (Tesla-inspired)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Tips and What We Didn't Include\n",
        "Tips: Always balance entropy with coherence—use BLEU scores alongside.\n\n",
        "Missed: Conditional entropy for context-dependent diversity; necessary for scientists to model dependencies like in Einstein's relativity (frame-dependent info).\n\n",
        "Next Steps: Read 'A Mathematical Theory of Communication' by Shannon; implement in full NLG pipeline with external libraries."
      ]
    }
  ]
}