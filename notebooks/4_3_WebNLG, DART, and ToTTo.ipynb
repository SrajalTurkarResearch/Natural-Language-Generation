{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive NLG Tutorial: WebNLG, DART, and ToTTo for Aspiring Scientists\n",
    "\n",
    "This Jupyter Notebook is a complete guide to understanding WebNLG, DART, and ToTTo, three key datasets in Natural Language Generation (NLG). Designed for beginners aiming to become scientists, it covers theory, practical code, visualizations, applications, research directions, and projects. We'll use simple language, analogies, and real-world examples to make concepts clear and note-friendly.\n",
    "\n",
    "## Objectives\n",
    "- Learn NLG and the role of WebNLG, DART, and ToTTo.\n",
    "- Implement practical code with Hugging Face Transformers.\n",
    "- Visualize dataset structures and model performance.\n",
    "- Explore real-world applications and case studies.\n",
    "- Identify research directions and project ideas.\n",
    "- Understand future steps and essential topics for scientists.\n",
    "\n",
    "## Prerequisites\n",
    "- Install Python packages: `pip install transformers torch pandas matplotlib seaborn datasets`\n",
    "- Basic Python knowledge (covered in resources like [Python-Knowledge-Hub](https://github.com/Nilesh-Choudhary/Python-Knowledge-Hub)).\n",
    "\n",
    "## Structure\n",
    "1. Theory of NLG and Datasets\n",
    "2. Practical Code: Model Training and Inference\n",
    "3. Visualizations\n",
    "4. Applications and Case Studies\n",
    "5. Mini and Major Project Ideas\n",
    "6. Research Directions and Rare Insights\n",
    "7. Future Directions and Tips\n",
    "8. Missing Topics for Scientists\n",
    "9. Conclusion\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theory of NLG and Datasets\n",
    "\n",
    "### What is NLG?\n",
    "Natural Language Generation (NLG) is an AI process that transforms data into human-readable text. Think of it as a storyteller who takes raw facts (like a table of weather data) and crafts a sentence like, \"It's sunny with a high of 75°F.\"\n",
    "\n",
    "**Analogy**: NLG is like a chef turning ingredients (data) into a tasty dish (text). The ingredients must be chosen wisely (content selection) and prepared well (surface realization).\n",
    "\n",
    "### Structured Data in NLG\n",
    "- **Tables**: Rows and columns, e.g., a spreadsheet of names and ages.\n",
    "- **RDF Triples**: Subject-predicate-object facts, e.g., \"Einstein-born_in-Germany.\"\n",
    "- **Key-Value Pairs**: Simple mappings, e.g., \"Temperature: 75°F.\"\n",
    "\n",
    "### WebNLG\n",
    "- **Description**: Dataset of RDF triples from DBpedia, generating text for 15 domains (e.g., Astronaut, City).\n",
    "- **Input**: 1–7 triples, e.g., `(Alan_Bean, occupation, Astronaut)`.\n",
    "- **Output**: Multi-sentence text, e.g., \"Alan Bean is an astronaut born in Wheeler, Texas.\"\n",
    "- **Challenge**: Generalizing to unseen domains.\n",
    "- **Size**: ~35,000 examples (WebNLG 2020).\n",
    "\n",
    "### DART\n",
    "- **Description**: Open-domain dataset combining WebNLG, Wikipedia tables, and more.\n",
    "- **Input**: RDF triples or tree ontologies (hierarchical data).\n",
    "- **Output**: Multi-sentence text, often complex.\n",
    "- **Challenge**: Handling diverse, hierarchical inputs.\n",
    "- **Size**: 82,191 examples.\n",
    "\n",
    "### ToTTo\n",
    "- **Description**: Table-to-text dataset from Wikipedia with highlighted cells.\n",
    "- **Input**: Table with selected cells, e.g., |Name|Occupation| → |Marie_Curie|Scientist|.\n",
    "- **Output**: Single sentence, e.g., \"Marie Curie was a scientist.\"\n",
    "- **Challenge**: Precise content selection.\n",
    "- **Size**: ~120,000 examples.\n",
    "\n",
    "**Analogy**: WebNLG is a librarian picking specific facts, DART is a novelist weaving diverse tales, ToTTo is an editor summarizing highlighted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Practical Code: Model Training and Inference\n",
    "\n",
    "We'll use the T5 model from Hugging Face to generate text from WebNLG, DART, and ToTTo-like inputs. We'll also fine-tune a model on a sample dataset.\n",
    "\n",
    "### Setup\n",
    "Ensure you have the required libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch pandas matplotlib seaborn datasets\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Dataset\n",
    "We'll use the `web_nlg` dataset from Hugging Face for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rishu\\anaconda3\\envs\\temp_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found web_nlg.py",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load WebNLG dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweb_nlg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelease_v3.0_en\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(dataset[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])  \u001b[38;5;66;03m# View a sample\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rishu\\anaconda3\\envs\\temp_env\\Lib\\site-packages\\datasets\\load.py:1392\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1388\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1391\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rishu\\anaconda3\\envs\\temp_env\\Lib\\site-packages\\datasets\\load.py:1132\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1142\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rishu\\anaconda3\\envs\\temp_env\\Lib\\site-packages\\datasets\\load.py:1031\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1026\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1027\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1028\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1030\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rishu\\anaconda3\\envs\\temp_env\\Lib\\site-packages\\datasets\\load.py:989\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    982\u001b[39m     api.hf_hub_download(\n\u001b[32m    983\u001b[39m         repo_id=path,\n\u001b[32m    984\u001b[39m         filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    987\u001b[39m         proxies=download_config.proxies,\n\u001b[32m    988\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[32m    991\u001b[39m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[32m    992\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision != \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset scripts are no longer supported, but found web_nlg.py"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load WebNLG dataset\n",
    "dataset = load_dataset('web_nlg', 'release_v3.0_en')\n",
    "print(dataset['train'][0])  # View a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with Pre-trained T5\n",
    "Let's generate text from a sample input using T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Sample inputs for WebNLG, DART, ToTTo\n",
    "inputs = [\n",
    "    'generate text: Alan_Bean | occupation | Astronaut | birthPlace | Wheeler_Texas',  # WebNLG\n",
    "    'generate text: Empire_State_Building | height | 443.2_meters | location | New_York_City',  # DART\n",
    "    'generate text: table | Name | Marie_Curie | Occupation | Scientist',  # ToTTo\n",
    "]\n",
    "\n",
    "# Generate text\n",
    "for input_text in inputs:\n",
    "    inputs_tokenized = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    outputs = model.generate(**inputs_tokenized, max_length=50, num_beams=5)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f'Input: {input_text}\\nOutput: {generated_text}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The outputs may be rough since `t5-small` isn't fine-tuned. For better results, we’ll fine-tune below.\n",
    "\n",
    "### Fine-Tuning T5 on WebNLG\n",
    "Let’s fine-tune T5 on a small subset of WebNLG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare a small training dataset\n",
    "train_data = dataset['train'].select(range(100))  # Use 100 examples\n",
    "def preprocess(example):\n",
    "    triples = ' | '.join([f\"{t['subject']} | {t['property']} | {t['object']}\" for t in example['modified_tripleset']])\n",
    "    text = example['lex']['text'][0]\n",
    "    return {'input_text': f'generate text: {triples}', 'target_text': text}\n",
    "\n",
    "train_data = train_data.map(preprocess)\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['input_text'], max_length=512, truncation=True, padding='max_length')\n",
    "    targets = tokenizer(examples['target_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenized_data = train_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./fine_tuned_t5')\n",
    "tokenizer.save_pretrained('./fine_tuned_t5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Fine-tuning on a full dataset requires more compute. This is a demo on 100 examples.\n",
    "\n",
    "### Evaluating with BLEU\n",
    "Let’s compute the BLEU score for a generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = ['Alan Bean is an astronaut born in Wheeler, Texas.']\n",
    "candidate = 'Alan Bean, an astronaut, was born in Wheeler, Texas.'\n",
    "score = sentence_bleu([ref.split() for ref in reference], candidate.split())\n",
    "print(f'BLEU Score: {score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations\n",
    "\n",
    "Let’s visualize dataset characteristics and model performance using Matplotlib and Seaborn, inspired by data visualization tutorials.\n",
    "[Data Visualization Basics with Matplotlib](https://github.com/Geoffrey-lab/Data-Visualization-with-Python/blob/main/Data%20Visualization%20Basics%20with%20Matplotlib.ipynb), [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb)\n",
    "\n",
    "### Dataset Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset sizes\n",
    "datasets = ['WebNLG', 'DART', 'ToTTo']\n",
    "sizes = [35000, 82191, 120000]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=datasets, y=sizes, palette='viridis')\n",
    "plt.title('Dataset Size Comparison')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothetical Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothetical BLEU scores\n",
    "bleu_scores = [65, 60, 70]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=datasets, y=bleu_scores, palette='magma')\n",
    "plt.title('Hypothetical BLEU Scores Across Datasets')\n",
    "plt.ylabel('BLEU Score (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applications and Case Studies\n",
    "\n",
    "### Applications\n",
    "- **WebNLG**:\n",
    "  - Automated journalism: Generating bios from DBpedia.\n",
    "  - Chatbots: Answering factual queries.\n",
    "  - E-commerce: Product descriptions.\n",
    "- **DART**:\n",
    "  - Travel apps: Destination guides from mixed data.\n",
    "  - Education: Study guides from Wikipedia tables.\n",
    "  - Business: Summarizing complex datasets.\n",
    "- **ToTTo**:\n",
    "  - Sports: Game summaries from stats tables.\n",
    "  - Wikipedia: Auto-updating articles.\n",
    "  - Dashboards: Text captions for charts.\n",
    "\n",
    "### Case Studies\n",
    "See the separate `Case_Studies.md` file for detailed examples, including:\n",
    "- WebNLG in news automation.\n",
    "- DART in travel apps.\n",
    "- ToTTo in sports analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mini and Major Project Ideas\n",
    "\n",
    "### Mini Projects\n",
    "1. **WebNLG Text Generator**:\n",
    "   - **Task**: Fine-tune T5 on WebNLG to generate text from 1–3 triples.\n",
    "   - **Steps**: Load dataset, preprocess triples, train model, evaluate with BLEU.\n",
    "   - **Outcome**: A model generating simple bios.\n",
    "2. **DART Summarizer**:\n",
    "   - **Task**: Create a script to summarize Wikipedia tables using DART.\n",
    "   - **Steps**: Parse table data, use a pre-trained model, generate summaries.\n",
    "   - **Outcome**: Summaries for educational content.\n",
    "3. **ToTTo Sentence Generator**:\n",
    "   - **Task**: Build a tool to generate sentences from highlighted table cells.\n",
    "   - **Steps**: Simulate ToTTo data, fine-tune a model, test on sports tables.\n",
    "   - **Outcome**: Sports game summaries.\n",
    "\n",
    "### Major Projects\n",
    "1. **Cross-Dataset NLG System**:\n",
    "   - **Task**: Develop a unified model for WebNLG, DART, and ToTTo.\n",
    "   - **Steps**: Combine datasets, fine-tune a large model (e.g., T5-base), evaluate generalization.\n",
    "   - **Outcome**: A versatile NLG system for multiple data types.\n",
    "2. **Interactive NLG Dashboard**:\n",
    "   - **Task**: Create a web app to input triples/tables and generate text.\n",
    "   - **Steps**: Use Flask/Streamlit, integrate a fine-tuned model, add visualizations.\n",
    "   - **Outcome**: A tool for journalists or educators.\n",
    "3. **New Evaluation Metric**:\n",
    "   - **Task**: Design a metric beyond BLEU (e.g., incorporating semantic similarity).\n",
    "   - **Steps**: Analyze limitations of BLEU, implement a new metric, test on datasets.\n",
    "   - **Outcome**: A research paper contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Research Directions and Rare Insights\n",
    "\n",
    "### Research Directions\n",
    "- **Generalization**: Improve models’ ability to handle unseen domains in WebNLG.\n",
    "- **Coherence**: Enhance multi-sentence coherence in DART outputs.\n",
    "- **Controlled Generation**: Optimize content selection in ToTTo for precision.\n",
    "- **Multimodal NLG**: Combine text and visuals (e.g., generate captions for charts).\n",
    "- **Ethical NLG**: Study biases in generated text (e.g., gender or cultural biases).\n",
    "\n",
    "### Rare Insights\n",
    "- **WebNLG**: Models often struggle with referring expressions (e.g., \"he\" vs. \"Alan Bean\") due to limited context. Research on coreference resolution could improve fluency.\n",
    "- **DART**: Hierarchical inputs (tree ontologies) are underutilized. Exploring graph neural networks could unlock better performance.\n",
    "- **ToTTo**: Highlighted cells oversimplify content selection. Real-world tables often lack such guidance, so studying implicit selection is a gap.\n",
    "\n",
    "**Tip**: Read papers like \"DART: Open-Domain Structured Data Record to Text Generation\" (arXiv:2007.02871) for deeper insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Future Directions and Tips\n",
    "\n",
    "### Future Directions\n",
    "- **Large Language Models**: Leverage models like LLaMA or GPT-4 for NLG tasks.\n",
    "- **Few-Shot Learning**: Train models with fewer examples for efficiency.\n",
    "- **Multilingual NLG**: Extend datasets to non-English languages.\n",
    "- **Real-Time NLG**: Develop systems for live data (e.g., sports scores).\n",
    "\n",
    "### Tips for Scientists\n",
    "- **Reproducibility**: Use Jupyter Notebooks for clear, shareable experiments. ([Jupyter Best Practices](https://github.com/chrisvoncsefalvay/jupyter-best-practices))\n",
    "- **Version Control**: Store code on GitHub for collaboration.\n",
    "- **Read Widely**: Follow arXiv and Papers With Code for NLG advancements.\n",
    "- **Experiment**: Start with small datasets, then scale to full WebNLG/DART/ToTTo.\n",
    "- **Network**: Join NLP communities on X or Reddit (e.g., r/MachineLearning).\n",
    "\n",
    "## 8. Missing Topics for Scientists\n",
    "\n",
    "The previous tutorial covered basics but missed some critical areas for researchers:\n",
    "- **Evaluation Metrics Beyond BLEU**: BLEU focuses on n-gram overlap but ignores semantics. Explore BLEURT or ROUGE for semantic similarity.\n",
    "- **Data Preprocessing**: Real-world data is messy. Learn to clean and normalize triples/tables.\n",
    "- **Model Interpretability**: Understand why models generate specific outputs (e.g., attention mechanisms).\n",
    "- **Ethical Considerations**: Address biases in training data (e.g., DBpedia’s coverage of certain demographics).\n",
    "- **Scalability**: Handling large datasets requires efficient data pipelines and compute resources.\n",
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "This notebook provides a foundation for mastering WebNLG, DART, and ToTTo. You’ve learned theory, coded a model, visualized data, explored applications, and identified research paths. As a scientist, continue experimenting, reading, and collaborating to push NLG boundaries.\n",
    "\n",
    "**Next Steps**:\n",
    "1. Download datasets from Hugging Face (`web_nlg`, `dart`, `totto`).\n",
    "2. Fine-tune a larger model (e.g., T5-base).\n",
    "3. Propose a research question, e.g., \"How can we improve coherence in DART?\"\n",
    "4. Share your work on GitHub or X to build your research profile.\n",
    "\n",
    "Happy researching!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
